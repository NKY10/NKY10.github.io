<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于值函数的方法 | openset的博客</title><meta name=keywords content="强化学习,Q-Learning,DQN"><meta name=description content="摘要：基于值函数的强化学习方法，包括Q-Learning和DQN算法原理。"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="基于值函数的方法"><meta property="og:description" content="摘要：基于值函数的强化学习方法，包括Q-Learning和DQN算法原理。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-08-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-08-28T00:00:00+00:00"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="Q-Learning"><meta property="article:tag" content="DQN"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于值函数的方法"><meta name=twitter:description content="摘要：基于值函数的强化学习方法，包括Q-Learning和DQN算法原理。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"强化学习","item":"https://nky10.github.io/theory/rl/"},{"@type":"ListItem","position":3,"name":"基于值函数的方法","item":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于值函数的方法","name":"基于值函数的方法","description":"摘要：基于值函数的强化学习方法，包括Q-Learning和DQN算法原理。","keywords":["强化学习","Q-Learning","DQN"],"articleBody":"一、Q-Learning 1. 前情回顾 状态价值函数与动作-状态价值函数存在如下关系：\n$$ V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a) $$$$ Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma V^\\pi(s')\\right] $$最优策略 $\\pi^*$ 是使得价值函数最大的策略：\n$$ \\pi^* = \\arg\\max_\\pi V^\\pi(s), \\quad \\forall s \\in S $$最优状态价值函数：\n$$ V^*(s) = \\max_\\pi V^\\pi(s) $$最优动作价值函数：\n$$ Q^*(s,a) = \\max_\\pi Q^\\pi(s,a) $$对于最优状态价值函数，我们有：\n$$ V^*(s) = \\max_a Q^*(s,a) $$将Q函数的定义代入：\n$$ V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma V^\\*(s')\\right] $$对于最优的动作价值函数：\n$$ Q^*(s,a) = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma \\max_{a'} Q^\\*(s',a')\\right] $$ $Q^*(s,a)$ 等于：从状态 s 执行动作 a 后，所有可能的下一个状态 s' 所带来的「即时奖励 + 后续最优回报」的期望值。\n这两个式子告诉我们，可以通过迭代计算的方法来更新价值函数。\n2. 算法原理 2.1 基本思想 在智能体与环境的交互中，我们并不知道状态转移概率 $P(s'|s,a)$ 以及奖励 $R(s,a,s')$ 的具体形式，Q-Learning算法通过采样来估计这个期望值。\n我们不再计算所有可能下一个状态 $s'$ 的加权平均，而是通过实际交互，即在状态 $s$ 执行动作 $a$ 到达状态 $s'$ 以及实际获得的奖励 $r$。\n此时的采样目标是：\n$$ r + \\gamma \\max_{a'} Q(s',a') $$注意这里只是当前的价值，不是最优策略下的价值。\n2.2 无偏估计 在给定最优函数 $Q^*$ 的前提下，采样目标 $r + \\gamma \\max_{a'} Q^*(s',a')$ 是 $Q^*(s,a)$ 的一个无偏估计，因为它的期望正好等于 $Q^*(s,a)$。\n无偏估计的定义：\n一个估计量 $X$ 是某个真值 $\\theta$ 的无偏估计，如果：$\\mathbb{E}[X] = \\theta$。\n也就是说，虽然每次采样得到的 $X$ 可能有偏差（比如某次运气好奖励很高），但长期平均来看，它等于真实值。\n证明：\n假设我们现在已经有最优的 $Q^\\*(s,a)$，并且我们从环境采样一次：\n$$ \\text{Sample Target} = r + \\gamma \\max_{a'} Q^\\*(s',a') $$采样目标的期望值：\n$$ \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s',a') \\mid s,a\\right] = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma \\max_{a'} Q^\\*(s',a')\\right] $$而这正是贝尔曼最优方程右边的内容，那么我们的单次采样能够近似期望值。\n但是，在学习过程中，我们使用的 $Q$ 是估计值，且存在函数逼近误差或采样噪声，因此实际的 TD 目标是有偏的。记住了，Q-learning是自举过程中会引入偏差。\n在随机逼近理论中，如果更新方向是目标值的无偏估计，算法就可以像梯度下降一样，逐步收敛到正确解。如果估计有系统性偏差（有偏），可能收敛到错误的值。\n虽然单次TD目标是有偏的，但随着学习进行，估计误差逐渐减小，偏差趋于零。在满足一定条件下（如充分探索、合适学习率），这种「渐近无偏 + 零均值噪声」的更新机制可以保证算法渐近收敛到最优Q值。\n2.3 时序差分学习（TD-error） 我们不需要一次性更新所有状态，而是每次交互后逐步调整：\n这个逐步交互的过程有点像卡尔曼滤波里计算残差。\n$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right] $$其中：\n$\\alpha$ 是学习率 ($0 \u003c \\alpha \\leq 1$)，需要满足Robbins-Monro 条件 $r + \\gamma \\max_{a'} Q(s',a')$ 是采样目标值(TD) $r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ 是时序差分误差(TD-error) Robbins-Monro 条件（罗宾斯-蒙罗条件）是一组关于学习率序列 $\\{a_t\\}$ 的数学条件，用于保证随机迭代算法能够收敛到目标值（例如最优 Q 值、最优参数等）。\n它最早由 Herbert Robbins 和 Sutton Monro 在 1951 年提出，用于解决「在噪声环境下求根」或「求期望值」的问题。后来被广泛应用于随机逼近（Stochastic Approximation）和强化学习中。\n设 $a_t$ 是第 $t$ 次更新时的学习率（可能依赖于状态-动作对的更新次数），要满足以下两个条件：\n$\\sum_{t=1}^{\\infty} a_t = \\infty$：含义是总「学习力量」是无限的。作用：确保我们有足够的「调整机会」，即使初始值很差，也能最终被修正。 $\\sum_{t=1}^{\\infty} a_t^2 \u003c \\infty$：学习率的「波动能量」是有限的，确保噪声的影响会逐渐被压制，不会导致持续震荡。 在实际编程中通常不会严格按 $1/t$ 调整学习率，因为：\n初期学习太慢 环境可能非平稳 用固定的0.1比较好，虽然不收敛，但是效果不错（qwen说的）。\n虽然固定学习率不满足 Robbins-Monro 收敛条件（会导致震荡），但在实际任务中（如游戏、控制），我们更关心快速学习和良好性能，而非严格收敛。因此常用固定学习率（如 0.1）或自适应方法（如 Adam）。\n于是，我们可以用这个实际采样结果来估计贝尔曼方程右边的值：\n期望近似：用单次采样近似期望值\n$$ \\mathbb{E}\\left[R + \\gamma \\max_{a'} Q^*(s',a')\\right] \\approx r + \\gamma \\max_{a'} Q(s',a') $$ 增量更新：使用增量式方法更新估计值\n$$ Q_{\\text{new}}(s,a) = (1-\\alpha)Q_{\\text{old}}(s,a) + \\alpha\\left[r + \\gamma \\max_{a'} Q(s',a')\\right] $$ 重新整理：\n$$ Q_{\\text{new}}(s,a) = Q_{\\text{old}}(s,a) + \\alpha\\left[r + \\gamma \\max_{a'} Q(s',a') - Q_{\\text{old}}(s,a)\\right] $$ 3. 算法流程 Q-learning 学习的是最优贪婪策略 $\\pi^*(a|s) = \\arg\\max_a Q(s,a)$，而探索使用的是另一个策略（如 $\\varepsilon$-greedy），二者分离，属于典型的 off-policy 方法。\n探索率（Exploration Rate）：指智能体在决策时选择随机动作（而非当前最优动作）的概率。\n最常见的实现是 $\\varepsilon$-greedy 策略：\n以概率 $\\varepsilon$：随机选择一个动作（探索） 以概率 $1-\\varepsilon$：选择当前 Q 值最高的动作（利用） 在实际训练中，通常会让探索率随时间逐渐减小。\nQ-Learning 算法流程 初始化 Q 表（例如，所有 $Q(s,a) = 0$）\n对每一轮 episode：\n从初始状态开始 当未到达终止状态时： 根据当前策略（如 $\\varepsilon$-greedy）选择动作 $a_t$ 为了在探索（exploration）和利用（exploitation）之间平衡，Q-Learning 通常使用 $\\varepsilon$-greedy 策略： 以概率 $\\varepsilon$ 随机选择动作（探索） 以概率 $1-\\varepsilon$ 选择当前 Q 值最大的动作（利用） 随着训练进行，$\\varepsilon$ 可以逐渐减小（$\\varepsilon$ 衰减），实现从探索为主到利用为主的过渡 执行动作，观察奖励 $r_{t+1}$ 和新状态 $s_{t+1}$ 使用 Q-Learning 更新公式更新 Q 值 将状态更新为 $s_{t+1}$ 重复直到 Q 值收敛或达到训练轮数\n4. 总结 Q-learning 是一种离线策略（off-policy）强化学习算法，通过时序差分学习（TD learning）来估计动作价值函数 $Q(s,a)$。它利用一定的行为策略（如 $\\varepsilon$-greedy）收集经验，但是学习一个贪婪的目标策略。算法通过最小化时序差分误差来迭代更新 Q 值。尽管 TD 更新在初期具有偏差，但它是收敛的，并能在策略评估中提供一致估计。\nQ-learning 的本质是：通过 off-policy 的 TD 控制，逐步逼近满足最优贝尔曼方程的动作价值函数。\n局限性 仅适合离散、低维度的空间，当状态-动作空间为高维向量或数量很多时，效率很低 总是在取最大价值下的动作，极易受到噪声的影响 策略是外挂的，难以优化 在一些奖励稀疏的环境下收敛缓慢 二、DQN Deep Q Network (DQN)\n1. 算法原理 与 Q-learning 一样，DQN 希望通过下面的式子迭代动作价值函数：\n$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right] $$Q-learning 在状态空间巨大（如图像像素）、连续或高维的情况下 Q 表爆炸，无法存储和更新。\n1.1 神经网络 DQN 通过用神经网络近似 Q 函数（进行特征提取）来解决无限（巨大）空间的问题，如图像用 CNN，高维度用 MLP：\n$$ Q(s,a|\\theta) \\approx Q^*(s,a) $$采用均方误差损失函数：\n$$ \\mathcal{L} = \\mathbb{E}\\left[\\text{target} - Q(s,a|\\theta)\\right]^2 = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s',a'|\\theta^-) - Q(s,a|\\theta)\\right]^2 $$然而还有两个问题要解决：\n由于 Q 是递推计算的，因此相邻数据具有高相关性，训练过程很震荡，容易陷入局部最优解 在训练神经网络时，我们通常使用随机梯度下降（SGD）或其变体（如 Adam），这些优化方法理论前提是：每个训练样本是独立同分布的 Q 是不断变化的，训练目标不固定，难以收敛 致命三角\n1.2 关键技术 DQN 采用两个策略来缓解上面的问题：\n经验回放（Experience Replay） 创建一个回放缓冲区（Replay Buffer）D，存储所有经验 $(s,a,r,s',\\text{done})$ 每个样本被多次使用，减少样本需求 每次更新时，从 D 中随机采样小批量训练网络 随机能够明显降低数据间的时间相关性 每个样本被多次使用，减少样本需求 DQN 用批量更新更稳定 目标网络（Target Network） 移动靶问题：\n均方误差损失函数中，目标值 target 与当前值 Q 都由 $\\theta$ 控制，而这个损失又去更新 $\\theta$，不容易收敛\nDQN 通过引入目标网络 $\\theta^-$，在一定步数之内不更新（保证这个阶段内不会出现大幅度的震荡）：\n目标网络的更新：每 $C$ 步之后用主网络（评估网络）$\\theta$ 进行替换 主网络的更新：$\\theta$ 每一步都会用均方误差损失去更新 2. 算法流程 1# DQN 算法伪代码 2 3初始化： 4 主Q网络：Q(s, a; θ) 5 目标Q网络：Q(s, a; θ⁻)，初始化 θ⁻ = θ 6 回放缓冲区 D，容量为 M 7 环境状态 s 8 9for episode = 1 to N do: 10 s = 环境初始状态 11 while s 不是终止状态 do: 12 1. 根据ε-greedy策略选择动作a： 13 - 以概率 ε 随机选择动作 14 - 以概率 1-ε 选择 a = argmax_a' Q(s, a'; θ) 15 16 2. 执行动作a，观察奖励r和新状态s' 17 18 3. 存储经验 (s, a, r, s', done) 到回放缓冲区D 19 20 4. 从D中随机采样一个小批量经验 (s_i, a_i, r_i, s'_i, done_i) 21 22 5. 计算目标Q值： 23 - 如果 s'_i 是终止状态： 24 y_i = r_i 25 - 否则： 26 y_i = r_i + γ * max_{a'} Q(s'_i, a'; θ⁻) 27 28 6. 计算损失函数（均方误差）： 29 L(θ) = E[(y_i - Q(s_i, a_i; θ))²] 30 31 7. 使用梯度下降更新主网络参数θ（如Adam优化器） 32 33 8. 每隔C步，更新目标网络参数：θ⁻ ← θ 34 35 9. s ← s' 36 end while 37end for DQN 改进算法 Double DQN: 解决 DQN 的过估计问题，性能提升显著 Dueling DQN: 分离「状态价值」和「优势」，更适合决策 Prioritized Experience Replay: 重要经验优先回放，加速学习 Noisy Nets: 用网络参数噪声替代 $\\varepsilon$-greedy，实现持续探索 ","wordCount":"644","inLanguage":"en","datePublished":"2025-08-28T00:00:00Z","dateModified":"2025-08-28T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">基于值函数的方法</h1><div class=post-meta><span title='2025-08-28 00:00:00 +0000 UTC'>August 28, 2025</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e4%b8%80q-learning aria-label=一、Q-Learning>一、Q-Learning</a><ul><li><a href=#1-%e5%89%8d%e6%83%85%e5%9b%9e%e9%a1%be aria-label="1. 前情回顾">1. 前情回顾</a></li><li><a href=#2-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 aria-label="2. 算法原理">2. 算法原理</a><ul><li><a href=#21-%e5%9f%ba%e6%9c%ac%e6%80%9d%e6%83%b3 aria-label="2.1 基本思想">2.1 基本思想</a></li><li><a href=#22-%e6%97%a0%e5%81%8f%e4%bc%b0%e8%ae%a1 aria-label="2.2 无偏估计">2.2 无偏估计</a></li><li><a href=#23-%e6%97%b6%e5%ba%8f%e5%b7%ae%e5%88%86%e5%ad%a6%e4%b9%a0td-error aria-label="2.3 时序差分学习（TD-error）">2.3 时序差分学习（TD-error）</a></li></ul></li><li><a href=#3-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b aria-label="3. 算法流程">3. 算法流程</a><ul><li><a href=#q-learning-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b aria-label="Q-Learning 算法流程">Q-Learning 算法流程</a></li></ul></li><li><a href=#4-%e6%80%bb%e7%bb%93 aria-label="4. 总结">4. 总结</a><ul><li><a href=#%e5%b1%80%e9%99%90%e6%80%a7 aria-label=局限性>局限性</a></li></ul></li></ul></li><li><a href=#%e4%ba%8cdqn aria-label=二、DQN>二、DQN</a><ul><li><a href=#1-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 aria-label="1. 算法原理">1. 算法原理</a><ul><li><a href=#11-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c aria-label="1.1 神经网络">1.1 神经网络</a></li><li><a href=#12-%e5%85%b3%e9%94%ae%e6%8a%80%e6%9c%af aria-label="1.2 关键技术">1.2 关键技术</a><ul><li><a href=#%e7%bb%8f%e9%aa%8c%e5%9b%9e%e6%94%beexperience-replay aria-label="经验回放（Experience Replay）">经验回放（Experience Replay）</a></li><li><a href=#%e7%9b%ae%e6%a0%87%e7%bd%91%e7%bb%9ctarget-network aria-label="目标网络（Target Network）">目标网络（Target Network）</a></li></ul></li></ul></li><li><a href=#2-%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b aria-label="2. 算法流程">2. 算法流程</a><ul><li><a href=#dqn-%e6%94%b9%e8%bf%9b%e7%ae%97%e6%b3%95 aria-label="DQN 改进算法">DQN 改进算法</a></li></ul></li></ul></li></ul></div></details></div><div class=post-content><h1 id=一q-learning>一、Q-Learning<a hidden class=anchor aria-hidden=true href=#一q-learning>#</a></h1><h2 id=1-前情回顾>1. 前情回顾<a hidden class=anchor aria-hidden=true href=#1-前情回顾>#</a></h2><p>状态价值函数与动作-状态价值函数存在如下关系：</p>$$ V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s,a) $$$$ Q^\pi(s,a) = \sum_{s'} P(s'|s,a)\left[R(s,a,s') + \gamma V^\pi(s')\right] $$<p>最优策略 $\pi^*$ 是使得价值函数最大的策略：</p>$$ \pi^* = \arg\max_\pi V^\pi(s), \quad \forall s \in S $$<p>最优状态价值函数：</p>$$ V^*(s) = \max_\pi V^\pi(s) $$<p>最优动作价值函数：</p>$$ Q^*(s,a) = \max_\pi Q^\pi(s,a) $$<p>对于最优状态价值函数，我们有：</p>$$ V^*(s) = \max_a Q^*(s,a) $$<p>将Q函数的定义代入：</p>$$ V^*(s) = \max_a \sum_{s'} P(s'|s,a)\left[R(s,a,s') + \gamma V^\*(s')\right] $$<p>对于最优的动作价值函数：</p>$$ Q^*(s,a) = \sum_{s'} P(s'|s,a)\left[R(s,a,s') + \gamma \max_{a'} Q^\*(s',a')\right] $$<blockquote><p>$Q^*(s,a)$ 等于：从状态 <strong>s</strong> 执行动作 <strong>a</strong> 后，所有可能的下一个状态 <strong>s'</strong> 所带来的「即时奖励 + 后续最优回报」的期望值。</p><p><strong>这两个式子告诉我们，可以通过迭代计算的方法来更新价值函数。</strong></p></blockquote><h2 id=2-算法原理>2. 算法原理<a hidden class=anchor aria-hidden=true href=#2-算法原理>#</a></h2><h3 id=21-基本思想>2.1 基本思想<a hidden class=anchor aria-hidden=true href=#21-基本思想>#</a></h3><p>在智能体与环境的交互中，我们并不知道状态转移概率 $P(s'|s,a)$ 以及奖励 $R(s,a,s')$ 的具体形式，Q-Learning算法<strong>通过采样来估计这个期望值</strong>。</p><p>我们不再计算所有可能下一个状态 $s'$ 的加权平均，而是通过实际交互，即在状态 $s$ 执行动作 $a$ 到达状态 $s'$ 以及实际获得的奖励 $r$。</p><p>此时的采样目标是：</p>$$ r + \gamma \max_{a'} Q(s',a') $$<p>注意这里只是当前的价值，不是最优策略下的价值。</p><h3 id=22-无偏估计>2.2 无偏估计<a hidden class=anchor aria-hidden=true href=#22-无偏估计>#</a></h3><p>在给定最优函数 $Q^*$ 的前提下，采样目标 $r + \gamma \max_{a'} Q^*(s',a')$ 是 $Q^*(s,a)$ 的一个无偏估计，因为它的期望正好等于 $Q^*(s,a)$。</p><blockquote><p><strong>无偏估计的定义：</strong></p><p>一个估计量 $X$ 是某个真值 $\theta$ 的无偏估计，如果：$\mathbb{E}[X] = \theta$。</p><p>也就是说，虽然每次采样得到的 $X$ 可能有偏差（比如某次运气好奖励很高），但<strong>长期平均</strong>来看，它等于真实值。</p><p><strong>证明：</strong></p><p>假设我们现在已经有最优的 $Q^\*(s,a)$，并且我们从环境采样一次：</p>$$ \text{Sample Target} = r + \gamma \max_{a'} Q^\*(s',a') $$<p>采样目标的期望值：</p>$$ \mathbb{E}\left[r + \gamma \max_{a'} Q^*(s',a') \mid s,a\right] = \sum_{s'} P(s'|s,a)\left[R(s,a,s') + \gamma \max_{a'} Q^\*(s',a')\right] $$<p>而这正是贝尔曼最优方程右边的内容，那么我们的单次采样能够近似期望值。</p><p>但是，在学习过程中，我们使用的 $Q$ 是估计值，且存在函数逼近误差或采样噪声，因此实际的 TD 目标是<strong>有偏的</strong>。记住了，Q-learning是自举过程中会引入偏差。</p></blockquote><p>在随机逼近理论中，如果更新方向是目标值的无偏估计，算法就可以像梯度下降一样，逐步收敛到正确解。如果估计有系统性偏差（有偏），可能收敛到错误的值。</p><p>虽然单次TD目标是有偏的，但随着学习进行，估计误差逐渐减小，偏差趋于零。在满足一定条件下（如充分探索、合适学习率），这种「渐近无偏 + 零均值噪声」的更新机制可以保证算法渐近收敛到最优Q值。</p><h3 id=23-时序差分学习td-error>2.3 时序差分学习（TD-error）<a hidden class=anchor aria-hidden=true href=#23-时序差分学习td-error>#</a></h3><p>我们不需要一次性更新所有状态，而是每次交互后逐步调整：</p><blockquote><p>这个逐步交互的过程有点像卡尔曼滤波里计算残差。</p></blockquote>$$ Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right] $$<p>其中：</p><ul><li>$\alpha$ 是学习率 ($0 < \alpha \leq 1$)，需要满足<strong>Robbins-Monro 条件</strong></li><li>$r + \gamma \max_{a'} Q(s',a')$ 是采样目标值(TD)</li><li>$r + \gamma \max_{a'} Q(s',a') - Q(s,a)$ 是时序差分误差(TD-error)</li></ul><blockquote><p><strong>Robbins-Monro 条件</strong>（罗宾斯-蒙罗条件）是一组关于学习率序列 $\{a_t\}$ 的数学条件，用于保证随机迭代算法能够收敛到目标值（例如最优 Q 值、最优参数等）。</p><p>它最早由 Herbert Robbins 和 Sutton Monro 在 1951 年提出，用于解决「在噪声环境下求根」或「求期望值」的问题。后来被广泛应用于随机逼近（Stochastic Approximation）和强化学习中。</p><p>设 $a_t$ 是第 $t$ 次更新时的学习率（可能依赖于状态-动作对的更新次数），要满足以下两个条件：</p><ol><li>$\sum_{t=1}^{\infty} a_t = \infty$：含义是总「学习力量」是无限的。作用：确保我们有足够的「调整机会」，即使初始值很差，也能最终被修正。</li><li>$\sum_{t=1}^{\infty} a_t^2 < \infty$：学习率的「波动能量」是有限的，确保噪声的影响会逐渐被压制，不会导致持续震荡。</li></ol><p>在实际编程中通常不会严格按 $1/t$ 调整学习率，因为：</p><ul><li>初期学习太慢</li><li>环境可能非平稳</li></ul><p>用固定的0.1比较好，虽然不收敛，但是效果不错（qwen说的）。</p><p>虽然固定学习率不满足 Robbins-Monro 收敛条件（会导致震荡），但在实际任务中（如游戏、控制），我们更关心快速学习和良好性能，而非严格收敛。因此常用固定学习率（如 0.1）或自适应方法（如 Adam）。</p></blockquote><p>于是，我们可以用这个实际采样结果来估计贝尔曼方程右边的值：</p><ul><li><p><strong>期望近似</strong>：用单次采样近似期望值</p>$$ \mathbb{E}\left[R + \gamma \max_{a'} Q^*(s',a')\right] \approx r + \gamma \max_{a'} Q(s',a') $$</li><li><p><strong>增量更新</strong>：使用增量式方法更新估计值</p>$$ Q_{\text{new}}(s,a) = (1-\alpha)Q_{\text{old}}(s,a) + \alpha\left[r + \gamma \max_{a'} Q(s',a')\right] $$</li><li><p><strong>重新整理</strong>：</p>$$ Q_{\text{new}}(s,a) = Q_{\text{old}}(s,a) + \alpha\left[r + \gamma \max_{a'} Q(s',a') - Q_{\text{old}}(s,a)\right] $$</li></ul><h2 id=3-算法流程>3. 算法流程<a hidden class=anchor aria-hidden=true href=#3-算法流程>#</a></h2><p>Q-learning 学习的是最优贪婪策略 $\pi^*(a|s) = \arg\max_a Q(s,a)$，而探索使用的是另一个策略（如 $\varepsilon$-greedy），二者分离，属于典型的 off-policy 方法。</p><blockquote><p><strong>探索率（Exploration Rate）</strong>：指智能体在决策时选择随机动作（而非当前最优动作）的概率。</p><p>最常见的实现是 $\varepsilon$-greedy 策略：</p><ul><li>以概率 $\varepsilon$：随机选择一个动作（探索）</li><li>以概率 $1-\varepsilon$：选择当前 Q 值最高的动作（利用）</li></ul><p>在实际训练中，通常会让探索率随时间逐渐减小。</p></blockquote><h3 id=q-learning-算法流程>Q-Learning 算法流程<a hidden class=anchor aria-hidden=true href=#q-learning-算法流程>#</a></h3><ol><li><p><strong>初始化 Q 表</strong>（例如，所有 $Q(s,a) = 0$）</p></li><li><p><strong>对每一轮 episode</strong>：</p><ol><li>从初始状态开始</li><li>当未到达终止状态时：<ul><li>根据当前策略（如 $\varepsilon$-greedy）选择动作 $a_t$</li><li>为了在探索（exploration）和利用（exploitation）之间平衡，Q-Learning 通常使用 $\varepsilon$-greedy 策略：<ul><li>以概率 $\varepsilon$ 随机选择动作（探索）</li><li>以概率 $1-\varepsilon$ 选择当前 Q 值最大的动作（利用）</li><li>随着训练进行，$\varepsilon$ 可以逐渐减小（$\varepsilon$ 衰减），实现从探索为主到利用为主的过渡</li></ul></li><li>执行动作，观察奖励 $r_{t+1}$ 和新状态 $s_{t+1}$</li><li>使用 Q-Learning 更新公式更新 Q 值</li><li>将状态更新为 $s_{t+1}$</li></ul></li></ol></li><li><p><strong>重复直到 Q 值收敛或达到训练轮数</strong></p></li></ol><h2 id=4-总结>4. 总结<a hidden class=anchor aria-hidden=true href=#4-总结>#</a></h2><p>Q-learning 是一种离线策略（off-policy）强化学习算法，通过时序差分学习（TD learning）来估计动作价值函数 $Q(s,a)$。它利用一定的行为策略（如 $\varepsilon$-greedy）收集经验，但是学习一个贪婪的目标策略。算法通过最小化时序差分误差来迭代更新 Q 值。尽管 TD 更新在初期具有偏差，但它是收敛的，并能在策略评估中提供一致估计。</p><p>Q-learning 的本质是：通过 off-policy 的 TD 控制，逐步逼近满足最优贝尔曼方程的动作价值函数。</p><h3 id=局限性>局限性<a hidden class=anchor aria-hidden=true href=#局限性>#</a></h3><ul><li>仅适合离散、低维度的空间，当状态-动作空间为高维向量或数量很多时，效率很低</li><li>总是在取最大价值下的动作，极易受到噪声的影响</li><li>策略是外挂的，难以优化</li><li>在一些奖励稀疏的环境下收敛缓慢</li></ul><h1 id=二dqn>二、DQN<a hidden class=anchor aria-hidden=true href=#二dqn>#</a></h1><p><strong>Deep Q Network (DQN)</strong></p><h2 id=1-算法原理>1. 算法原理<a hidden class=anchor aria-hidden=true href=#1-算法原理>#</a></h2><p>与 Q-learning 一样，DQN 希望通过下面的式子迭代动作价值函数：</p>$$ Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma \max_{a'} Q(s',a') - Q(s,a)\right] $$<p>Q-learning 在状态空间巨大（如图像像素）、连续或高维的情况下 Q 表爆炸，无法存储和更新。</p><h3 id=11-神经网络>1.1 神经网络<a hidden class=anchor aria-hidden=true href=#11-神经网络>#</a></h3><p>DQN 通过用神经网络近似 Q 函数（进行特征提取）来解决无限（巨大）空间的问题，如图像用 CNN，高维度用 MLP：</p>$$ Q(s,a|\theta) \approx Q^*(s,a) $$<p>采用<strong>均方误差损失函数</strong>：</p>$$ \mathcal{L} = \mathbb{E}\left[\text{target} - Q(s,a|\theta)\right]^2 = \mathbb{E}\left[r + \gamma \max_{a'} Q^*(s',a'|\theta^-) - Q(s,a|\theta)\right]^2 $$<p>然而还有两个问题要解决：</p><ol><li>由于 Q 是递推计算的，因此相邻数据具有高相关性，训练过程很震荡，容易陷入局部最优解<ul><li>在训练神经网络时，我们通常使用随机梯度下降（SGD）或其变体（如 Adam），这些优化方法理论前提是：每个训练样本是独立同分布的</li></ul></li><li>Q 是不断变化的，训练目标不固定，难以收敛</li></ol><blockquote><p><a href=https://o9vao24145.feishu.cn/docx/Uz2SdlWMxojKUYxbTnrcmid3nrc#share-QxkndumSdoXH6rxWNEjcEYvdn6f>致命三角</a></p></blockquote><h3 id=12-关键技术>1.2 关键技术<a hidden class=anchor aria-hidden=true href=#12-关键技术>#</a></h3><p>DQN 采用两个策略来缓解上面的问题：</p><h4 id=经验回放experience-replay>经验回放（Experience Replay）<a hidden class=anchor aria-hidden=true href=#经验回放experience-replay>#</a></h4><ul><li>创建一个<strong>回放缓冲区（Replay Buffer）D</strong>，存储所有经验 $(s,a,r,s',\text{done})$<ul><li>每个样本被多次使用，减少样本需求</li></ul></li><li>每次更新时，从 D 中<strong>随机采样小批量</strong>训练网络<ul><li>随机能够明显降低数据间的时间相关性</li><li>每个样本被多次使用，减少样本需求</li><li>DQN 用批量更新更稳定</li></ul></li></ul><h4 id=目标网络target-network>目标网络（Target Network）<a hidden class=anchor aria-hidden=true href=#目标网络target-network>#</a></h4><blockquote><p><strong>移动靶问题：</strong></p><p>均方误差损失函数中，目标值 target 与当前值 Q 都由 $\theta$ 控制，而这个损失又去更新 $\theta$，不容易收敛</p></blockquote><p>DQN 通过引入目标网络 $\theta^-$，在一定步数之内不更新（保证这个阶段内不会出现大幅度的震荡）：</p><ul><li><strong>目标网络的更新</strong>：每 $C$ 步之后用主网络（评估网络）$\theta$ 进行替换</li><li><strong>主网络的更新</strong>：$\theta$ 每一步都会用均方误差损失去更新</li></ul><h2 id=2-算法流程>2. 算法流程<a hidden class=anchor aria-hidden=true href=#2-算法流程>#</a></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1</span><span><span style=color:#57606a># DQN 算法伪代码</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3</span><span>初始化<span style=color:#f6f8fa;background-color:#82071e>：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4</span><span>    主Q网络<span style=color:#f6f8fa;background-color:#82071e>：</span>Q<span style=color:#1f2328>(</span>s<span style=color:#1f2328>,</span> a<span style=color:#1f2328>;</span> θ<span style=color:#1f2328>)</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5</span><span>    目标Q网络<span style=color:#f6f8fa;background-color:#82071e>：</span>Q<span style=color:#1f2328>(</span>s<span style=color:#1f2328>,</span> a<span style=color:#1f2328>;</span> θ<span style=color:#f6f8fa;background-color:#82071e>⁻</span><span style=color:#1f2328>)</span><span style=color:#f6f8fa;background-color:#82071e>，</span>初始化 θ<span style=color:#f6f8fa;background-color:#82071e>⁻</span> <span style=color:#0550ae>=</span> θ
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6</span><span>    回放缓冲区 D<span style=color:#f6f8fa;background-color:#82071e>，</span>容量为 M
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7</span><span>    环境状态 s
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8</span><span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9</span><span><span style=color:#cf222e>for</span> episode <span style=color:#0550ae>=</span> <span style=color:#0550ae>1</span> to N do<span style=color:#1f2328>:</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10</span><span>    s <span style=color:#0550ae>=</span> 环境初始状态
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11</span><span>    <span style=color:#cf222e>while</span> s 不是终止状态 do<span style=color:#1f2328>:</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">12</span><span>        <span style=color:#0550ae>1.</span> 根据ε<span style=color:#0550ae>-</span>greedy策略选择动作a<span style=color:#f6f8fa;background-color:#82071e>：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">13</span><span>           <span style=color:#0550ae>-</span> 以概率 ε 随机选择动作
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">14</span><span>           <span style=color:#0550ae>-</span> 以概率 <span style=color:#0550ae>1</span><span style=color:#0550ae>-</span>ε 选择 a <span style=color:#0550ae>=</span> argmax_a<span style=color:#0a3069>&#39; Q(s, a&#39;</span><span style=color:#1f2328>;</span> θ<span style=color:#1f2328>)</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">15</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">16</span><span>        <span style=color:#0550ae>2.</span> 执行动作a<span style=color:#f6f8fa;background-color:#82071e>，</span>观察奖励r和新状态s<span style=color:#0a3069>&#39;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">17</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">18</span><span>        <span style=color:#0550ae>3.</span> 存储经验 <span style=color:#1f2328>(</span>s<span style=color:#1f2328>,</span> a<span style=color:#1f2328>,</span> r<span style=color:#1f2328>,</span> s<span style=color:#0a3069>&#39;, done) 到回放缓冲区D</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">19</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">20</span><span>        <span style=color:#0550ae>4.</span> 从D中随机采样一个小批量经验 <span style=color:#1f2328>(</span>s_i<span style=color:#1f2328>,</span> a_i<span style=color:#1f2328>,</span> r_i<span style=color:#1f2328>,</span> s<span style=color:#0a3069>&#39;_i, done_i)</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">21</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">22</span><span>        <span style=color:#0550ae>5.</span> 计算目标Q值<span style=color:#f6f8fa;background-color:#82071e>：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">23</span><span>           <span style=color:#0550ae>-</span> 如果 s<span style=color:#0a3069>&#39;_i 是终止状态：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">24</span><span>               y_i <span style=color:#0550ae>=</span> r_i
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">25</span><span>           <span style=color:#0550ae>-</span> 否则<span style=color:#f6f8fa;background-color:#82071e>：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">26</span><span>               y_i <span style=color:#0550ae>=</span> r_i <span style=color:#0550ae>+</span> γ <span style=color:#0550ae>*</span> max_<span style=color:#1f2328>{</span>a<span style=color:#0a3069>&#39;} Q(s&#39;</span>_i<span style=color:#1f2328>,</span> a<span style=color:#0a3069>&#39;; θ⁻)</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">27</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">28</span><span>        <span style=color:#0550ae>6.</span> 计算损失函数<span style=color:#f6f8fa;background-color:#82071e>（</span>均方误差<span style=color:#f6f8fa;background-color:#82071e>）：</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">29</span><span>               L<span style=color:#1f2328>(</span>θ<span style=color:#1f2328>)</span> <span style=color:#0550ae>=</span> E<span style=color:#1f2328>[(</span>y_i <span style=color:#0550ae>-</span> Q<span style=color:#1f2328>(</span>s_i<span style=color:#1f2328>,</span> a_i<span style=color:#1f2328>;</span> θ<span style=color:#1f2328>))</span><span style=color:#f6f8fa;background-color:#82071e>²</span><span style=color:#1f2328>]</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">30</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">31</span><span>        <span style=color:#0550ae>7.</span> 使用梯度下降更新主网络参数θ<span style=color:#f6f8fa;background-color:#82071e>（</span>如Adam优化器<span style=color:#f6f8fa;background-color:#82071e>）</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">32</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">33</span><span>        <span style=color:#0550ae>8.</span> 每隔C步<span style=color:#f6f8fa;background-color:#82071e>，</span>更新目标网络参数<span style=color:#f6f8fa;background-color:#82071e>：</span>θ<span style=color:#f6f8fa;background-color:#82071e>⁻</span> <span style=color:#f6f8fa;background-color:#82071e>←</span> θ
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">34</span><span>        
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">35</span><span>        <span style=color:#0550ae>9.</span> s <span style=color:#f6f8fa;background-color:#82071e>←</span> s<span style=color:#0a3069>&#39;</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">36</span><span>    end <span style=color:#cf222e>while</span>
</span></span><span style=display:flex><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">37</span><span>end <span style=color:#cf222e>for</span>
</span></span></code></pre></div><h3 id=dqn-改进算法>DQN 改进算法<a hidden class=anchor aria-hidden=true href=#dqn-改进算法>#</a></h3><ul><li><strong>Double DQN</strong>: 解决 DQN 的过估计问题，性能提升显著</li><li><strong>Dueling DQN</strong>: 分离「状态价值」和「优势」，更适合决策</li><li><strong>Prioritized Experience Replay</strong>: 重要经验优先回放，加速学习</li><li><strong>Noisy Nets</strong>: 用网络参数噪声替代 $\varepsilon$-greedy，实现持续探索</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></li><li><a href=https://nky10.github.io/tags/q-learning/>Q-Learning</a></li><li><a href=https://nky10.github.io/tags/dqn/>DQN</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>