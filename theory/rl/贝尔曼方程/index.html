<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>贝尔曼方程 | openset的博客</title><meta name=keywords content="强化学习,贝尔曼方程"><meta name=description content="摘要：贝尔曼方程是强化学习中用于计算状态价值函数的重要工具，通过递归定义，将问题分解为子问题，最终求解最优策略。"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="贝尔曼方程"><meta property="og:description" content="摘要：贝尔曼方程是强化学习中用于计算状态价值函数的重要工具，通过递归定义，将问题分解为子问题，最终求解最优策略。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-09-02T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-02T00:00:00+00:00"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="贝尔曼方程"><meta name=twitter:card content="summary"><meta name=twitter:title content="贝尔曼方程"><meta name=twitter:description content="摘要：贝尔曼方程是强化学习中用于计算状态价值函数的重要工具，通过递归定义，将问题分解为子问题，最终求解最优策略。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"强化学习","item":"https://nky10.github.io/theory/rl/"},{"@type":"ListItem","position":3,"name":"贝尔曼方程","item":"https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"贝尔曼方程","name":"贝尔曼方程","description":"摘要：贝尔曼方程是强化学习中用于计算状态价值函数的重要工具，通过递归定义，将问题分解为子问题，最终求解最优策略。","keywords":["强化学习","贝尔曼方程"],"articleBody":"1. 简化模型分析：无策略下的状态价值函数 在引入策略和动作之前，先考虑一个简化的随机游走模型（即不涉及策略 $\\pi$ 和动作 a），各状态按概率转移，目标是分析状态的价值。\n基本定义 即时奖励期望： $R(s) = \\mathbb{E}[R_{t+1} \\mid S_t = s]$ 表示从状态 $s$ 出发时立即获得的奖励期望。 回报（Return）： $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ 其中 $\\gamma \\in [0,1)$ 是折扣因子，用于衡量未来奖励的重要性。 状态价值函数： $V(s) = \\mathbb{E}[G_t \\mid S_t = s]$ 表示从状态 $s$ 开始，遵循当前环境动态所能获得的长期期望回报。 示例说明 以如下状态转移图为例：\n暂时无法在飞书文档外展示此内容\n假设在时刻 $t$ 处于状态 B，则其价值为：$V(B) = p \\cdot (R_C + \\gamma R_D) + (1 - p) \\cdot R_E$\n这体现了状态 $ s $ 的潜在长期收益，包含即时奖励与后续折现奖励。\n2. 推导状态价值函数的贝尔曼方程 我们从定义出发，推导 $V(s)$ 的递推形式。\n$$ \\begin{aligned} V(s) \u0026= \\mathbb{E}[G_t \\mid S_t = s] \\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] \\\\ \u0026= \\mathbb{E}[R_{t+1} \\mid S_t = s] + \\gamma \\mathbb{E}[G_{t+1} \\mid S_t = s] \\\\ \u0026= R(s) + \\gamma \\mathbb{E}[G_{t+1} \\mid S_t = s] \\end{aligned} $$ ✅ 注释：此步利用了期望的线性性质。\n进一步处理 $\\mathbb{E}[G_{t+1} \\mid S_t = s]$：\n$$ \\begin{aligned} \\mathbb{E}[G_{t+1} \\mid S_t = s] \u0026= \\sum_{s'} P(s' \\mid s) \\cdot \\mathbb{E}[G_{t+1} \\mid S_t = s, S_{t+1} = s'] \\\\ \u0026= \\sum_{s'} P(s' \\mid s) \\cdot \\mathbb{E}[G_{t+1} \\mid S_{t+1} = s'] \\quad \\text{（马尔可夫性质）} \\\\ \u0026= \\sum_{s'} P(s' \\mid s) \\cdot V(s') \\end{aligned} $$ ✅ 关键点：马尔可夫性质保证了未来期望仅依赖于当前状态，与历史无关。\n代入得最终结果：\n$$ \\boxed{ V(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s) V(s') } $$3. 贝尔曼方程组（引入策略 $\\pi$） 当引入策略 $\\pi(a \\mid s)$ 后，需区分状态价值函数 $V_\\pi(s)$ 和动作价值函数 $Q_\\pi(a, s)$。\n定义：\n$V_\\pi(s)$：在策略 $\\pi$ 下，从状态 $ s $ 出发的期望回报。 $Q_\\pi(a, s)$$：在状态 $$ s $执行动作 $a$ 后，再遵循策略$\\pi$的期望回报。 贝尔曼方程组： $$ \\left\\{ \\begin{aligned} V_\\pi(s) \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q_\\pi(a, s) \\\\ Q_\\pi(a, s) \u0026= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} P(s', r \\mid a, s) \\left[ r + \\gamma V_\\pi(s') \\right] \\end{aligned} \\right. $$4. 贝尔曼最优方程 最优策略的核心思想\n每一步选择都应最大化未来的期望回报，即 贪心策略在每一步都追求最大值。\n最优价值函数定义\n$V^*(s)$：所有策略中最大的状态价值。 $Q^*(a, s)$：所有策略中最大的动作价值。 贝尔曼最优方程 $$ \\left\\{ \\begin{aligned} V^*(s) \u0026= \\max_a Q^*(a, s) \\\\ Q^*(a, s) \u0026= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} P(s', r \\mid a, s) \\left[ r + \\gamma V^*(s') \\right] \\end{aligned} \\right. $$或等价地：\n$$ V^*(s) = \\max_a \\sum_{s'} P(s' \\mid a, s) \\left[ R(s, a, s') + \\gamma V^*(s') \\right] $$总结 概念 公式 说明 状态价值函数（无策略） $V(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s) V(s')$ 基础贝尔曼方程 状态价值函数（有策略） $V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(a, s)$ 策略下期望回报 动作价值函数 $Q_\\pi(a, s) = \\sum_{s'} P(s' \\mid a, s) [ r + \\gamma V_\\pi(s') ]$ 动作执行后期望回报 最优价值函数 $V^*(s) = \\max_a Q^*(a, s)$ 最大可能价值 贝尔曼最优方程 $Q^*(a, s) = \\sum_{s'} P(s' \\mid a, s) [ r + \\gamma V^*(s') ]$ 最优策略基础 这些方程构成了强化学习中动态规划、策略迭代、值迭代等方法的理论基石。\n手推过程 ","wordCount":"374","inLanguage":"en","datePublished":"2025-09-02T00:00:00Z","dateModified":"2025-09-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">贝尔曼方程</h1><div class=post-meta><span title='2025-09-02 00:00:00 +0000 UTC'>September 2, 2025</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e7%ae%80%e5%8c%96%e6%a8%a1%e5%9e%8b%e5%88%86%e6%9e%90%e6%97%a0%e7%ad%96%e7%95%a5%e4%b8%8b%e7%9a%84%e7%8a%b6%e6%80%81%e4%bb%b7%e5%80%bc%e5%87%bd%e6%95%b0 aria-label="1. 简化模型分析：无策略下的状态价值函数">1. 简化模型分析：无策略下的状态价值函数</a><ul><li><a href=#%e5%9f%ba%e6%9c%ac%e5%ae%9a%e4%b9%89 aria-label=基本定义>基本定义</a></li><li><a href=#%e7%a4%ba%e4%be%8b%e8%af%b4%e6%98%8e aria-label=示例说明>示例说明</a></li></ul></li><li><a href=#2-%e6%8e%a8%e5%af%bc%e7%8a%b6%e6%80%81%e4%bb%b7%e5%80%bc%e5%87%bd%e6%95%b0%e7%9a%84%e8%b4%9d%e5%b0%94%e6%9b%bc%e6%96%b9%e7%a8%8b aria-label="2. 推导状态价值函数的贝尔曼方程">2. 推导状态价值函数的贝尔曼方程</a></li><li><a href=#3-%e8%b4%9d%e5%b0%94%e6%9b%bc%e6%96%b9%e7%a8%8b%e7%bb%84%e5%bc%95%e5%85%a5%e7%ad%96%e7%95%a5- aria-label="3. 贝尔曼方程组（引入策略 $\pi$）">3. 贝尔曼方程组（引入策略 $\pi$）</a></li><li><a href=#4-%e8%b4%9d%e5%b0%94%e6%9b%bc%e6%9c%80%e4%bc%98%e6%96%b9%e7%a8%8b aria-label="4. 贝尔曼最优方程">4. 贝尔曼最优方程</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e6%89%8b%e6%8e%a8%e8%bf%87%e7%a8%8b aria-label=手推过程>手推过程</a></li></ul></div></details></div><div class=post-content><h2 id=1-简化模型分析无策略下的状态价值函数>1. 简化模型分析：无策略下的状态价值函数<a hidden class=anchor aria-hidden=true href=#1-简化模型分析无策略下的状态价值函数>#</a></h2><p>在引入策略和动作之前，先考虑一个简化的随机游走模型（即不涉及策略 $\pi$ 和动作 a），各状态按概率转移，目标是分析状态的价值。</p><h3 id=基本定义>基本定义<a hidden class=anchor aria-hidden=true href=#基本定义>#</a></h3><ul><li><strong>即时奖励期望</strong>：<ul><li>$R(s) = \mathbb{E}[R_{t+1} \mid S_t = s]$</li><li>表示从状态 $s$ 出发时立即获得的奖励期望。</li></ul></li><li><strong>回报（Return）</strong>：<ul><li>$G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</li><li>其中 $\gamma \in [0,1)$ 是折扣因子，用于衡量未来奖励的重要性。</li></ul></li><li><strong>状态价值函数</strong>：<ul><li>$V(s) = \mathbb{E}[G_t \mid S_t = s]$</li><li>表示从状态 $s$ 开始，遵循当前环境动态所能获得的长期期望回报。</li></ul></li></ul><h3 id=示例说明>示例说明<a hidden class=anchor aria-hidden=true href=#示例说明>#</a></h3><p>以如下状态转移图为例：</p><p>暂时无法在飞书文档外展示此内容</p><p>假设在时刻 $t$ 处于状态 B，则其价值为：$V(B) = p \cdot (R_C + \gamma R_D) + (1 - p) \cdot R_E$</p><p>这体现了状态 $ s $ 的潜在长期收益，包含即时奖励与后续折现奖励。</p><h2 id=2-推导状态价值函数的贝尔曼方程>2. 推导状态价值函数的贝尔曼方程<a hidden class=anchor aria-hidden=true href=#2-推导状态价值函数的贝尔曼方程>#</a></h2><p>我们从定义出发，推导 $V(s)$ 的递推形式。</p>$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t \mid S_t = s] \\
&= \mathbb{E}[R_{t+1} + \gamma G_{t+1} \mid S_t = s] \\
&= \mathbb{E}[R_{t+1} \mid S_t = s] + \gamma \mathbb{E}[G_{t+1} \mid S_t = s] \\
&= R(s) + \gamma \mathbb{E}[G_{t+1} \mid S_t = s]
\end{aligned}
$$<blockquote><p>✅ <strong>注释</strong>：此步利用了<strong>期望的线性性质</strong>。</p></blockquote><p>进一步处理 $\mathbb{E}[G_{t+1} \mid S_t = s]$：</p>$$
\begin{aligned}
\mathbb{E}[G_{t+1} \mid S_t = s] &= \sum_{s'} P(s' \mid s) \cdot \mathbb{E}[G_{t+1} \mid S_t = s, S_{t+1} = s'] \\
&= \sum_{s'} P(s' \mid s) \cdot \mathbb{E}[G_{t+1} \mid S_{t+1} = s'] \quad \text{（马尔可夫性质）} \\
&= \sum_{s'} P(s' \mid s) \cdot V(s')
\end{aligned}
$$<blockquote><p>✅ <strong>关键点</strong>：马尔可夫性质保证了未来期望仅依赖于当前状态，与历史无关。</p></blockquote><p>代入得最终结果：</p>$$
\boxed{ V(s) = R(s) + \gamma \sum_{s'} P(s' \mid s) V(s') }
$$<h2 id=3-贝尔曼方程组引入策略->3. 贝尔曼方程组（引入策略 $\pi$）<a hidden class=anchor aria-hidden=true href=#3-贝尔曼方程组引入策略->#</a></h2><p>当引入策略 $\pi(a \mid s)$ 后，需区分<strong>状态价值函数</strong> $V_\pi(s)$ 和<strong>动作价值函数</strong> $Q_\pi(a, s)$。</p><p>定义：</p><ul><li>$V_\pi(s)$：在策略 $\pi$ 下，从状态 $ s $ 出发的期望回报。</li><li>$Q_\pi(a, s)$$：在状态 $$ s $执行动作 $a$ 后，再遵循策略$\pi$的期望回报。</li></ul><p>贝尔曼方程组：</p>$$
\left\{
\begin{aligned}
V_\pi(s) &= \sum_{a \in \mathcal{A}} \pi(a \mid s) Q_\pi(a, s) \\
Q_\pi(a, s) &= \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} P(s', r \mid a, s) \left[ r + \gamma V_\pi(s') \right]
\end{aligned}
\right.
$$<h2 id=4-贝尔曼最优方程>4. 贝尔曼最优方程<a hidden class=anchor aria-hidden=true href=#4-贝尔曼最优方程>#</a></h2><p>最优策略的核心思想</p><blockquote><p>每一步选择都应最大化未来的期望回报，即 <strong>贪心策略在每一步都追求最大值</strong>。</p></blockquote><p>最优价值函数定义</p><ul><li>$V^*(s)$：所有策略中最大的状态价值。</li><li>$Q^*(a, s)$：所有策略中最大的动作价值。</li></ul><p>贝尔曼最优方程</p>$$
\left\{
\begin{aligned}
V^*(s) &= \max_a Q^*(a, s) \\
Q^*(a, s) &= \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} P(s', r \mid a, s) \left[ r + \gamma V^*(s') \right]
\end{aligned}
\right.
$$<p>或等价地：</p>$$
V^*(s) = \max_a \sum_{s'} P(s' \mid a, s) \left[ R(s, a, s') + \gamma V^*(s') \right]
$$<h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><table><thead><tr><th>概念</th><th>公式</th><th>说明</th></tr></thead><tbody><tr><td><strong>状态价值函数（无策略）</strong></td><td>$V(s) = R(s) + \gamma \sum_{s'} P(s' \mid s) V(s')$</td><td>基础贝尔曼方程</td></tr><tr><td><strong>状态价值函数（有策略）</strong></td><td>$V_\pi(s) = \sum_a \pi(a \mid s) Q_\pi(a, s)$</td><td>策略下期望回报</td></tr><tr><td><strong>动作价值函数</strong></td><td>$Q_\pi(a, s) = \sum_{s'} P(s' \mid a, s) [ r + \gamma V_\pi(s') ]$</td><td>动作执行后期望回报</td></tr><tr><td><strong>最优价值函数</strong></td><td>$V^*(s) = \max_a Q^*(a, s)$</td><td>最大可能价值</td></tr><tr><td><strong>贝尔曼最优方程</strong></td><td>$Q^*(a, s) = \sum_{s'} P(s' \mid a, s) [ r + \gamma V^*(s') ]$</td><td>最优策略基础</td></tr></tbody></table><p>这些方程构成了强化学习中<strong>动态规划、策略迭代、值迭代</strong>等方法的理论基石。</p><p><img alt=img loading=lazy src="https://o9vao24145.feishu.cn/space/api/box/stream/download/asynccode/?code=M2VhZTljMjZiMjJlZjg2MzY0OWRjYjFkZWJlMTBiNzZfSmFGVndzeDgwWGwxeVVudHowVTY2ZXVhU0t4RjhnNTdfVG9rZW46Q2U4QmJ2VWxvbzlSYWZ4cG5YV2NaWG9HbndlXzE3NTc3NTUzNDE6MTc1Nzc1ODk0MV9WNA"></p><h2 id=手推过程>手推过程<a hidden class=anchor aria-hidden=true href=#手推过程>#</a></h2><p><img alt=img loading=lazy src="https://o9vao24145.feishu.cn/space/api/box/stream/download/asynccode/?code=YmExYjU4M2MwMmJlNGIwZjJjMDUwYTBhOTM2Mjc2ZWNfV2hTYXJKeHFrN2dMUnZNVVBxVDBDdHJ5Q21RSm9TVnVfVG9rZW46THkyOWJNTGFmb3NLb2l4bVpTVWNCT3B2blBnXzE3NTc3NTUzNDE6MTc1Nzc1ODk0MV9WNA"></p><p><img alt=img loading=lazy src="https://o9vao24145.feishu.cn/space/api/box/stream/download/asynccode/?code=MWYyMjY0MDIzMDM3M2E0NjUwMDhmYmNlOGYyYWMzMzBfZUk5SmpZODdXZ0VlRmkzNk1lYWJ5dHc3dEtkQ3BpdXNfVG9rZW46VElKUmJIcnpybzFZSUt4Nnc4cGN5TWhWbjlmXzE3NTc3NTUzNDE6MTc1Nzc1ODk0MV9WNA"></p><p><img alt=img loading=lazy src="https://o9vao24145.feishu.cn/space/api/box/stream/download/asynccode/?code=NTAxYTIwNzE5Y2NlMTExMzA0YzMyOWFiZDgxOTEyNjVfZlI4SzhvMDZLbFAwYXFjbmk1eFVRTFl4UHVqVnNKRUVfVG9rZW46UFRUWWI0cnZtb1BCVmR4VlNSS2NOVWZIbkxkXzE3NTc3NTUzNDE6MTc1Nzc1ODk0MV9WNA"></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></li><li><a href=https://nky10.github.io/tags/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/>贝尔曼方程</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>