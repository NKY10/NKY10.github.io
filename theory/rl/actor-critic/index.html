<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Actor-Critic | openset的博客</title><meta name=keywords content="强化学习,REINFORCE,AC"><meta name=description content="摘要：演员评论家框架的分析"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/rl/actor-critic/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/rl/actor-critic/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/rl/actor-critic/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="Actor-Critic"><meta property="og:description" content="摘要：演员评论家框架的分析"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-09-16T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-16T00:00:00+00:00"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="REINFORCE"><meta property="article:tag" content="AC"><meta name=twitter:card content="summary"><meta name=twitter:title content="Actor-Critic"><meta name=twitter:description content="摘要：演员评论家框架的分析"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"强化学习","item":"https://nky10.github.io/theory/rl/"},{"@type":"ListItem","position":3,"name":"Actor-Critic","item":"https://nky10.github.io/theory/rl/actor-critic/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Actor-Critic","name":"Actor-Critic","description":"摘要：演员评论家框架的分析","keywords":["强化学习","REINFORCE","AC"],"articleBody":"核心思想 REINFORCE是策略梯度定理最直接的应用，通过蒙特卡洛方法采样轨迹来计算整个轨迹的回报。但这存在着明显的缺点：\n高方差： $G_t$ 依赖于从当前状态开始一系列的动作和状态转移，随机性很大。不同的回合，即使状态和动作相同，$G_t$也可能相差巨大。高方差会导致训练不稳定，收敛缓慢。 效率低：必须等回合结束， 无法进行单步更新，学习效率低。 Actor-Critic框架通过引入一个评论家Critic来解决了REINFORCE的痛点\nActor是策略函数$\\pi(a|s;\\theta)$，由参数的影响下根据环境做出动作 Critic是价值函数（可以是Q也可以是V，通常用V），根据Actor做出的动作的结果$(s, a, r, s')$来更新对状态价值的判断 Actor根据这个反馈来更新自己的策略，决定未来是更倾向于还是更避免做这个动作 反馈是用$$\\text{TD-error}$$，衡量的是当前Critic预测 $V(s)$ 和更优的估计 $r + γ * V(s')$ 之间的差距, 相当于$V(s)$自举更新，但是用真实的奖励不断进行校正了. $\\delta_t = r + \\gamma V(s') - V(s) $ 为什么Critic通常使用$V(s)$而不是$Q(s,a)$？\n策略梯度定理中提到引入基线$V(s)$可以降低方差，即用$Q(s,a)-V(s)$\n这就是优势函数$A(s,a)=Q(s,a)-V(s)$\n而在策略$\\pi$下，$Q(s,a)=E[r+\\gamma V(s')]$，因此在单次采样下，可以用$\\delta_t$近似估计$A(s,a)$\n所以只学习$V(s)$，就能近似计算出优势函数，不用专门学习更复杂的$Q(s,a)$\nAC具体流程 Actor在状态$s$做出动作$a$，并根据环境反馈得到奖励$r$和新状态$s'$ Critic根据当前状态$s$和动作$a$，做出自己对这一步的判断，即预测当前状态的价值$V(s)$ Critic根据新状态$s'$和奖励$r$，预测新状态的价值$V(s')$ 计算TD-error：$$\\delta_t = r + \\gamma * V(s') - V(s)$$ 更新Critic的参数，使得预测的价值更接近真实的奖励加上未来的价值：$$V(s) \\leftarrow V(s) + \\alpha * \\delta_t$$ 更新Actor的参数，使得在当前状态下，选择这个动作的概率增加：$$\\theta \\leftarrow \\theta + \\beta * \\delta_t * \\nabla_a \\log \\pi(a|s;\\theta)$$ 重复1-6，直到收敛 Asynchronous Advantage Actor-Critic (A3C) 证明了异步分布式训练和无经验回放算法的可行性\nA3C（异步）的工作流程：\n创建多个环境： 同样运行N个环境副本。\n创建多个线程： 每个worker在一个独立的CPU线程上运行。（这是关键！）\n异步采样： 每个worker拥有主网络参数的本地副本。它们不同步地、独立地从主网络拷贝参数，然后用自己的环境交互若干步。\n异步计算更新： 每个worker根据自己的经验独立地计算梯度。\n异步更新： 计算完成后，每个worker立即将自己的梯度传回给主网络。主网络在不加锁的情况下，持续地用收到的梯度更新自己的参数。\nAdvantage Actor-Critic (A2C) 在AC算法中，我们用TD-error估计优势函数$A(s,a)$，即\n$$\\delta_t = r + γ * V(s') - V(s) \\approx A(s, a)$$虽然是低方差的，但是属于有偏估计。\nA2C采用**n步回报（n-step return）**估计：\n$$A(s,a)\\approx G_t^{(n)}-V(s)$$其中$G_t^{(n)}$是n步回报，$G_t^{(n)}=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\gamma^n V(s_{t+n})$\n当n=1时，就是AC，低方差有偏 当n=∞时，就是MC，高方差无偏 n处于中间，就在MC和AC之间，方差和偏置都在平衡 实际实现中，A2C通常泛指那些使用多个并行环境并同步更新的AC算法。\nA2C的优势：\n降低方差： 通过并行采样，多个worker在不同状态探索，收集到的经验是解相关（decorrelated） 的。将这些经验一起求平均更新，相当于大大降低了梯度的方差。这比在单个环境中顺序采样的方差要小得多。\n更稳定： 同步更新避免了不同步的参数带来的混乱，训练过程更稳定。\n效率高： 在GPU上，对一批并行数据做前向和反向传播远比串行处理一个个环境高效。\n","wordCount":"123","inLanguage":"en","datePublished":"2025-09-16T00:00:00Z","dateModified":"2025-09-16T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/rl/actor-critic/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Actor-Critic</h1><div class=post-meta><span title='2025-09-16 00:00:00 +0000 UTC'>September 16, 2025</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label=核心思想>核心思想</a></li><li><a href=#ac%e5%85%b7%e4%bd%93%e6%b5%81%e7%a8%8b aria-label=AC具体流程>AC具体流程</a></li><li><a href=#asynchronous-advantage-actor-critic-a3c aria-label="Asynchronous Advantage Actor-Critic (A3C)">Asynchronous Advantage Actor-Critic (A3C)</a></li><li><a href=#advantage-actor-critic-a2c aria-label="Advantage Actor-Critic (A2C)">Advantage Actor-Critic (A2C)</a></li></ul></div></details></div><div class=post-content><h2 id=核心思想>核心思想<a hidden class=anchor aria-hidden=true href=#核心思想>#</a></h2><p>REINFORCE是<a href=https://o9vao24145.feishu.cn/docx/Uz2SdlWMxojKUYxbTnrcmid3nrc#share-CRbndDmUJoIhZvxHMsMckZkknjg>策略梯度定理</a>最直接的应用，通过蒙特卡洛方法采样轨迹来计算整个轨迹的回报。但这存在着明显的缺点：</p><ul><li>高方差： $G_t$ 依赖于从当前状态开始一系列的动作和状态转移，随机性很大。不同的回合，即使状态和动作相同，$G_t$也可能相差巨大。高方差会导致训练不稳定，收敛缓慢。</li><li>效率低：必须等回合结束， 无法进行单步更新，学习效率低。</li></ul><p>Actor-Critic框架通过引入一个评论家Critic来解决了REINFORCE的痛点</p><ul><li>Actor是策略函数$\pi(a|s;\theta)$，由参数的影响下根据环境做出动作</li><li>Critic是价值函数（可以是Q也可以是V，通常用V），根据Actor做出的动作的结果$(s, a, r, s')$来更新对状态价值的判断</li><li>Actor根据这个反馈来更新自己的策略，决定未来是更倾向于还是更避免做这个动作</li><li>反馈是用$$\text{TD-error}$$，衡量的是当前Critic预测 $V(s)$ 和更优的估计 $r + γ * V(s')$ 之间的差距, 相当于$V(s)$自举更新，但是用真实的奖励不断进行校正了. $\delta_t = r + \gamma V(s') - V(s) $</li></ul><blockquote><p>为什么Critic通常使用$V(s)$而不是$Q(s,a)$？</p><p>策略梯度定理中提到引入基线$V(s)$可以降低方差，即用$Q(s,a)-V(s)$</p><blockquote><p>这就是优势函数$A(s,a)=Q(s,a)-V(s)$</p></blockquote><p>而在策略$\pi$下，$Q(s,a)=E[r+\gamma V(s')]$，因此在单次采样下，可以用$\delta_t$近似估计$A(s,a)$</p><p>所以只学习$V(s)$，就能近似计算出优势函数，不用专门学习更复杂的$Q(s,a)$</p></blockquote><h2 id=ac具体流程>AC具体流程<a hidden class=anchor aria-hidden=true href=#ac具体流程>#</a></h2><ol><li>Actor在状态$s$做出动作$a$，并根据环境反馈得到奖励$r$和新状态$s'$</li><li>Critic根据当前状态$s$和动作$a$，做出自己对这一步的判断，即预测当前状态的价值$V(s)$</li><li>Critic根据新状态$s'$和奖励$r$，预测新状态的价值$V(s')$</li><li>计算TD-error：$$\delta_t = r + \gamma * V(s') - V(s)$$</li><li>更新Critic的参数，使得预测的价值更接近真实的奖励加上未来的价值：$$V(s) \leftarrow V(s) + \alpha * \delta_t$$</li><li>更新Actor的参数，使得在当前状态下，选择这个动作的概率增加：$$\theta \leftarrow \theta + \beta * \delta_t * \nabla_a \log \pi(a|s;\theta)$$</li><li>重复1-6，直到收敛</li></ol><h2 id=asynchronous-advantage-actor-critic-a3c>Asynchronous Advantage Actor-Critic (A3C)<a hidden class=anchor aria-hidden=true href=#asynchronous-advantage-actor-critic-a3c>#</a></h2><blockquote><p>证明了异步分布式训练和无经验回放算法的可行性</p></blockquote><p>A3C（异步）的工作流程：</p><ul><li><p>创建多个环境： 同样运行N个环境副本。</p></li><li><p>创建多个线程： 每个worker在一个独立的CPU线程上运行。（这是关键！）</p></li><li><p>异步采样： 每个worker拥有主网络参数的本地副本。它们不同步地、独立地从主网络拷贝参数，然后用自己的环境交互若干步。</p></li><li><p>异步计算更新： 每个worker根据自己的经验独立地计算梯度。</p></li><li><p>异步更新： 计算完成后，每个worker立即将自己的梯度传回给主网络。主网络在不加锁的情况下，持续地用收到的梯度更新自己的参数。</p></li></ul><h2 id=advantage-actor-critic-a2c>Advantage Actor-Critic (A2C)<a hidden class=anchor aria-hidden=true href=#advantage-actor-critic-a2c>#</a></h2><p>在AC算法中，我们用TD-error估计优势函数$A(s,a)$，即</p>$$\delta_t = r + γ * V(s') - V(s) \approx A(s, a)$$<p>虽然是低方差的，但是属于有偏估计。</p><p>A2C采用**n步回报（n-step return）**估计：</p>$$A(s,a)\approx G_t^{(n)}-V(s)$$<p>其中$G_t^{(n)}$是n步回报，$G_t^{(n)}=r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}+\cdots+\gamma^{n-1} r_{t+n-1}+\gamma^n V(s_{t+n})$</p><ul><li>当n=1时，就是AC，低方差有偏</li><li>当n=∞时，就是MC，高方差无偏</li><li>n处于中间，就在MC和AC之间，方差和偏置都在平衡</li></ul><blockquote><p>实际实现中，A2C通常泛指那些使用多个并行环境并同步更新的AC算法。</p></blockquote><p>A2C的优势：</p><ul><li><p>降低方差： 通过并行采样，多个worker在不同状态探索，收集到的经验是解相关（decorrelated） 的。将这些经验一起求平均更新，相当于大大降低了梯度的方差。这比在单个环境中顺序采样的方差要小得多。</p></li><li><p>更稳定： 同步更新避免了不同步的参数带来的混乱，训练过程更稳定。</p></li><li><p>效率高： 在GPU上，对一批并行数据做前向和反向传播远比串行处理一个个环境高效。</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></li><li><a href=https://nky10.github.io/tags/reinforce/>REINFORCE</a></li><li><a href=https://nky10.github.io/tags/ac/>AC</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>