<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>基于策略梯度的方法 | openset的博客</title><meta name=keywords content="强化学习,REINFORCE,策略梯度定理,致命三角"><meta name=description content="摘要：推导策略梯度定理，介绍REINFORCE的基本原理，分析致命三角问题。"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="基于策略梯度的方法"><meta property="og:description" content="摘要：推导策略梯度定理，介绍REINFORCE的基本原理，分析致命三角问题。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-09-10T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-10T00:00:00+00:00"><meta property="article:tag" content="强化学习"><meta property="article:tag" content="REINFORCE"><meta property="article:tag" content="策略梯度定理"><meta property="article:tag" content="致命三角"><meta name=twitter:card content="summary"><meta name=twitter:title content="基于策略梯度的方法"><meta name=twitter:description content="摘要：推导策略梯度定理，介绍REINFORCE的基本原理，分析致命三角问题。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"强化学习","item":"https://nky10.github.io/theory/rl/"},{"@type":"ListItem","position":3,"name":"基于策略梯度的方法","item":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"基于策略梯度的方法","name":"基于策略梯度的方法","description":"摘要：推导策略梯度定理，介绍REINFORCE的基本原理，分析致命三角问题。","keywords":["强化学习","REINFORCE","策略梯度定理","致命三角"],"articleBody":"1. 引言 基于价值函数的方法（如 Q-learning、DQN）通过估计动作价值函数 $Q(s,a)$ ，并选择使 Q 值最大的动作来进行决策。\n由于需要计算 $arg\\max_{a}Q(s,a)$ ，这类方法天然适用于离散动作空间。虽然 DQN 能处理连续状态空间，但其动作空间必须是离散且有限的； 策略是通过Q间接得到结果的，不够灵活； 探索依赖$\\epsilon -greedy$, 需要一些方法去平衡探索过程，容易限制性能（2. gym-cartpole验证过）。 尽管存在一些扩展方法（如 DDPG、QR-DQN）尝试将价值函数应用于连续动作空间，但在高维或复杂动作空间（如大型语言模型生成 token 的分布）中，这类方法通常不实用\n这时候我们考虑用神经网络对策略函数进行建模，通过梯度更新的方法学习策略网络\n这样的好处在于：\n$\\theta$策略本身就是随机的，探索过程更优 它能够解决连续动作空间和状态空间的问题 2. 核心思想 用一个神经网络来表示策略，参数为$\\theta$\n$$\\pi_\\theta(a|s)=P(a|s;\\theta)$$ $\\pi_\\theta(a|s)$： 策略，由参数 $\\theta$ 控制（比如神经网络的权重）。 $\\tau=(s_0,a_0,r_1,s_1,a_1,r2,...)$: 一条轨迹，代表从开始到结束的一个完整交互序列 $G(\\tau)$: 这个序列的总回报 $P(\\tau;\\theta)$: 策略$\\pi_\\theta$下完成轨迹$\\tau$的概率 我们的目标是找到最优$\\theta^*$,使得智能体与环境交互后的总回报最大, 即\n$$J(\\theta)=E_{\\tau～\\pi_\\theta}[G(\\tau)]=\\sum_\\tau P(\\tau;\\theta)·G(\\tau)$$ $J(\\theta)$是所有可能轨迹的回报，按其发生的概率进行加权平均。一个“好”的策略会让高回报的轨迹出现概率高，从而拉高这个平均值。\n**问题求解：**想要$J(\\theta)$最大，沿着梯度上升的方向更新参数 $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$\n3. 策略梯度定理（Policy Gradient Theorem）推导 对$J(\\theta)$进行求导得 $\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left[ \\sum_{\\tau} P(\\tau; \\theta) G(\\tau) \\right]$\n式字中$G(\\tau)$由环境决定，为常量，与$\\theta$无关，因此重点在于如何求轨迹概率分布$P(\\tau;\\theta)$的导数。\n计算一个概率分布的梯度：\n取log后求导\n$$\\nabla_\\theta \\log P(\\tau; \\theta) = \\frac{\\nabla_\\theta P(\\tau; \\theta)}{P(\\tau; \\theta)}$$, 因此\n$$\\nabla_\\theta P(\\tau; \\theta) = P(\\tau; \\theta) \\nabla_\\theta \\log P(\\tau; \\theta)$$ 求导\n$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\sum_{\\tau} P(\\tau; \\theta) G(\\tau) = \\sum_{\\tau}\\nabla_\\theta P(\\tau; \\theta) G(\\tau) =\\sum_{\\tau}\\nabla_\\theta P(\\tau; \\theta) \\nabla_\\theta \\log P(\\tau; \\theta)G(\\tau)$$得到中间结果\n$${\\color{blue} \\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log P(\\tau; \\theta) \\cdot G(\\tau) \\right] \\quad }$$轨迹的概率分布由每一步的状态和动作决定\n$$P(\\tau; \\theta) = p(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t | s_t) p(s_{t+1} | s_t, a_t)$$ 前面乘$p(s_0)$是考虑随机初始状态的情况\n取对数，连乘变成了求和\n$$\\log P(\\tau; \\theta) = \\log p(s_0) + \\sum_{t=0}^{T} \\left[ \\log \\pi_\\theta(a_t | s_t) + \\log p(s_{t+1} | s_t, a_t) \\right]$$求导，常数项都没了，可得\n$$\\nabla_\\theta \\log P(\\tau; \\theta) = \\nabla_\\theta \\left[ \\log p(s_0) + ... \\right] = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)$$$${\\color{red}\\nabla_\\theta J(\\theta)= E_{\\tau \\sim \\pi_\\theta} [ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G(\\tau)]={\\boxed {E_{\\tau \\sim \\pi_\\theta} [ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t]}}}$$ $G(\\tau)$是完成整条轨迹的回报，而在时间步\n$$t$$之前的行为已经发生。\n策略梯度定理告诉我们：不需要知道环境模型（即状态转移概率 $p(s'|s, a)$），只需要根据当前策略$\\pi_\\theta$ 与环境交互得到的样本轨迹$\\tau$，就能计算出优化策略所需梯度的无偏估计。\n很伟大，为$\\nabla_\\theta J(\\theta)$这个黑箱提供了可计算的方法\n直观理解：如果一个动作 ($a_t$) 之后获得了很高的回报 ($G_t$)，我们就大幅调整参数，使得未来在类似状态 ($s_t$) 下选择这个动作的概率大大增加。反之，如果回报很低，我们就降低选择它的概率。\n上面的推导过程是在有限轨迹的任务中，但实际问题中轨迹空间很大，因此需要一个一般形式的策略梯度定理\n用价值函数替代轨迹的回报，因为\n$$Q^{\\pi_\\theta}(s,a) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\mid s_t = s, a_t = a \\right]$$$${\\boxed {{\\color{red} \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s,a) \\right]}}}$$ REINFORCE算法 核心思路：基于**策略梯度定理****，用蒙特卡洛方法估计策略梯度。**\n进行采样的理由：\n策略梯度定理给出的是期望形式的梯度，需要对所有可能轨迹或所有状态-动作对求期望。 但在实际环境中，状态和轨迹空间巨大甚至无限，无法穷举计算，因此必须通过采样轨迹进行蒙特卡洛估计。 蒙特卡洛方法：\n通过“随机采样”来估计数学量（如期望、积分、概率等）的数值计算方法，多次采样求平均作为期望。\n在强化学习中，蒙特卡洛方法通过完整轨迹的回报（return）来估计价值函数或策略梯度，不依赖环境模型或自举（bootstrapping）。\nREINFORCE的优点：\n无偏估计（Unbiased）：采样越多，估计越准（大数定律保证），没有模型误差或自举偏差。 模型无关：不需要知道环境转移概率 $p(s'∣s,a)$，也不需要学习价值函数 实现简单、概念清晰：采样轨迹 → 计算算回报 → 计算算梯度 → 更新参数 在足够采样和小学习率下，能收敛到局部最优（在策略梯度框架内） 策略是随机的，天然支持探索（对比 ε-greedy 的硬切换） 重点来了🌟：\n然而，由于使用完整轨迹的总回报$G_t$作为价值估计，且未利用任何函数近似或基线（baseline），即使在长期采样下期望是无偏估计，但单次采样的梯度噪声极大，**REINFORCE 的梯度估计方差极高，**会导致训练过程不稳定、收敛缓慢，尤其在长轨迹或稀疏奖励环境中表现更差。\n后面的Actor-Critic、TRPO、PPO基本都是在解决大方差的问题\nActor-Critic结构引入Critic 网络作为价值函数的估计以减小方差 TRPO用KL散度限制每次策略更新的距离，因为即使梯度方向对，一步更新太大也会导致策略“崩溃”——因为新策略下的数据分布变了，旧梯度不再适用 PPO则直接裁剪梯度，算是对TRPO对工程优化 关于强化学习中期望估计的方法总结： 可以从两个维度分类：\n按是否使用“自举”（Bootstrapping） → 蒙特卡洛 vs. 时序差分 按是否使用“函数逼近” → 表格式 vs. 神经网络/线性逼近 常见的有下面几种\n方法 全称 核心思想 是否自举 是否无偏 方差 样本效率 适用场景 MC 蒙特卡洛（Monte Carlo） 用完整轨迹的实际回报估计期望 ❌ 否 ✅ 是 ⬆️ 高 ❌ 低 Episodic任务，无需模型 TD 时序差分（Temporal Difference） 用一步预测 + 下一状态估计来更新 ✅ 是 ❌ 否（有偏） ⬇️ 低 ✅ 高 在线学习，continuing任务 TD(λ) / GAE 多步时序差分 / 广义优势估计 MC 与 TD 的折中，加权多步回报 ✅ 是 ❌ 近似无偏（λ→1） 🟡 中 ✅ 高 策略梯度、Actor-Critic MC的方差大，前面提到了。\nTD的方差小，是因为估计过程是逐步进行的，两步之间的相关性比较高。但在 DQN 中，这种相关性 + 自举 + 函数逼近导致训练不稳定，必须用BufferReplay等技巧去除相关性。\nTD(λ) / GAE还没学，看了Actor-Critic之后再回来分析\n致命三角Deadly Triad： “致命三角”指的是函数逼近、自举和离线策略学习三者结合时可能导致的不收敛问题。\n离线策略（Off-Policy）： 用来生成行为（收集数据）的策略（行为策略 β(a|s)）与我们要学习改进的策略（目标策略 π(a|s)）是不同的。 自举（Bootstrapping）：在价值函数学习中，自举是指使用下一个状态（或动作）的“当前估计值”来计算当前状态（或动作）的目标值（target）。 函数逼近（Function Approximation)：使用参数化函数（如神经网络）来近似值函数$V(s)$或$Q(s, a)$$或策略函数$$π(a|s)$。多数时候如此因为状态空间太大或连续，无法用表格表示。 函数逼近 + 自举 + 离线策略= 训练不稳定\n这三个因素中的任意一个单独出现，通常都是可管理的。但当它们结合时，会产生一种危险的正反馈循环：\n初始误差： 由于函数逼近，我们的值函数初始就存在误差。 误差传播与放大： 自举过程会将这些误差从后续状态（s'）传播回当前状态（s）。更糟糕的是，在离策略学习中，我们用于自举的数据（来自行为策略β）可能与目标策略π下的真实数据分布不符，这往往会系统性高估或低估某些值，从而放大这些误差。 目标漂移： 由于我们的更新目标是基于有误差的估计值，而这个目标本身也在不断移动（因为我们在不断更新网络）。这导致我们像是在追逐一个移动的、不准的靶子。 发散： 最终，这种误差的传播和放大可能导致值函数的估计彻底失控，变得无穷大（发散）或者收敛到一个完全错误的值。 $Q-learning$ + $Q(\\theta)$。\nQ-learning是Off-Policy的，因为收集数据的策略是$ε-greedy$，学习是贪婪策略的Q值 Q-learning的自举过程引入了估计偏差（因为目标值依赖当前不完美的 Q 估计），在表格法 + 无限采样 + 合适学习率条件下，Q 值可收敛到无偏最优值，即Q-learning本身是可收敛的 但是引入神经网络对Q进行估计时，如果某个 Q 值被高估，那下一步目标也被高估，错误放大从而导致训练发散 所以DQN采用目标网络（Target Network），延迟更新目标 Q 值，打断自举导致的正反馈循环，防止 Q 值爆炸性增长。 一个简单的比喻：想象你在教一个机器人走路（目标策略π），但为了收集数据，你让另一个醉酒的机器人（行为策略β）去到处晃悠。\n函数逼近： 你用一个简单的模型来总结醉酒机器人的经验。 自举： 机器人根据这个不完美的模型来做决策（“如果我从这里迈一步，根据模型估计，我可能会摔倒”）。 离线策略： 你用醉酒机器人的数据来教清醒机器人走路。 ","wordCount":"419","inLanguage":"en","datePublished":"2025-09-10T00:00:00Z","dateModified":"2025-09-10T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">基于策略梯度的方法</h1><div class=post-meta><span title='2025-09-10 00:00:00 +0000 UTC'>September 10, 2025</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-%e5%bc%95%e8%a8%80 aria-label="1. 引言">1. 引言</a></li><li><a href=#2-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3 aria-label="2. 核心思想">2. 核心思想</a></li><li><a href=#3-%e7%ad%96%e7%95%a5%e6%a2%af%e5%ba%a6%e5%ae%9a%e7%90%86policy-gradient-theorem%e6%8e%a8%e5%af%bc aria-label="3. 策略梯度定理（Policy Gradient Theorem）推导">3. 策略梯度定理（Policy Gradient Theorem）推导</a></li><li><a href=#reinforce%e7%ae%97%e6%b3%95 aria-label=REINFORCE算法>REINFORCE算法</a></li><li><a href=#%e5%85%b3%e4%ba%8e%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e4%b8%ad%e6%9c%9f%e6%9c%9b%e4%bc%b0%e8%ae%a1%e7%9a%84%e6%96%b9%e6%b3%95%e6%80%bb%e7%bb%93 aria-label=关于强化学习中期望估计的方法总结：>关于强化学习中期望估计的方法总结：</a></li><li><a href=#%e8%87%b4%e5%91%bd%e4%b8%89%e8%a7%92deadly-triad aria-label="致命三角Deadly Triad：">致命三角Deadly Triad：</a></li></ul></div></details></div><div class=post-content><h2 id=1-引言>1. 引言<a hidden class=anchor aria-hidden=true href=#1-引言>#</a></h2><p>基于价值函数的方法（如 Q-learning、DQN）通过估计动作价值函数 $Q(s,a)$ ，并选择使 Q 值最大的动作来进行决策。</p><ul><li>由于需要计算 $arg\max_{a}Q(s,a)$ ，这类方法天然适用于离散动作空间。虽然 DQN 能处理连续状态空间，但其动作空间必须是离散且有限的；</li><li>策略是通过Q间接得到结果的，不够灵活；</li><li>探索依赖$\epsilon -greedy$, 需要一些方法去平衡探索过程，容易限制性能（<a href=https://o9vao24145.feishu.cn/docx/Q6uyd44XsopZVYxAdm7cBpXvnQh>2. gym-cartpole</a>验证过）。</li></ul><blockquote><p>尽管存在一些扩展方法（如 DDPG、QR-DQN）尝试将价值函数应用于连续动作空间，但在高维或复杂动作空间（如大型语言模型生成 token 的分布）中，这类方法通常不实用</p></blockquote><p>这时候我们考虑用神经网络对策略函数进行建模，通过梯度更新的方法学习策略网络</p><blockquote><p>这样的好处在于：</p><ul><li>$\theta$策略本身就是随机的，探索过程更优</li><li>它能够解决连续动作空间和状态空间的问题</li></ul></blockquote><h2 id=2-核心思想>2. 核心思想<a hidden class=anchor aria-hidden=true href=#2-核心思想>#</a></h2><p>用一个神经网络来表示策略，参数为$\theta$</p>$$\pi_\theta(a|s)=P(a|s;\theta)$$<ul><li>$\pi_\theta(a|s)$： 策略，由参数 $\theta$ 控制（比如神经网络的权重）。</li><li>$\tau=(s_0,a_0,r_1,s_1,a_1,r2,...)$: 一条轨迹，代表从开始到结束的一个完整交互序列</li><li>$G(\tau)$: 这个序列的总回报</li><li>$P(\tau;\theta)$: 策略$\pi_\theta$下完成轨迹$\tau$的概率</li></ul><p><strong>我们的目标是找到最优</strong>$\theta^*$<strong>,使得<strong><strong>智能体</strong></strong>与环境交互后的总回报最大,</strong> 即</p>$$J(\theta)=E_{\tau～\pi_\theta}[G(\tau)]=\sum_\tau P(\tau;\theta)·G(\tau)$$<blockquote><p>$J(\theta)$是所有可能轨迹的回报，按其发生的概率进行加权平均。一个“好”的策略会让高回报的轨迹出现概率高，从而拉高这个平均值。</p></blockquote><p>**问题求解：**想要$J(\theta)$最大，沿着梯度上升的方向更新参数 $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$</p><h2 id=3-策略梯度定理policy-gradient-theorem推导>3. 策略梯度定理（Policy Gradient Theorem）推导<a hidden class=anchor aria-hidden=true href=#3-策略梯度定理policy-gradient-theorem推导>#</a></h2><p>对$J(\theta)$进行求导得 $\nabla_\theta J(\theta) = \nabla_\theta \left[ \sum_{\tau} P(\tau; \theta) G(\tau) \right]$</p><p>式字中$G(\tau)$由环境决定，为常量，与$\theta$无关，因此<strong>重点在于如何求轨迹概率分布</strong>$P(\tau;\theta)$<strong>的导数</strong>。</p><blockquote><p>计算一个概率分布的梯度：</p><p><strong>取<strong><strong>log</strong></strong>后求导</strong></p>$$\nabla_\theta \log P(\tau; \theta) = \frac{\nabla_\theta P(\tau; \theta)}{P(\tau; \theta)}$$<p>, 因此</p>$$\nabla_\theta P(\tau; \theta) = P(\tau; \theta) \nabla_\theta \log P(\tau; \theta)$$</blockquote><p>求导</p>$$\nabla_\theta J(\theta) = \nabla_\theta \sum_{\tau} P(\tau; \theta) G(\tau) = \sum_{\tau}\nabla_\theta P(\tau; \theta) G(\tau) =\sum_{\tau}\nabla_\theta P(\tau; \theta) \nabla_\theta \log P(\tau; \theta)G(\tau)$$<p>得到中间结果</p>$${\color{blue} \nabla_\theta J(\theta) = E_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log P(\tau; \theta) \cdot G(\tau) \right] \quad }$$<p>轨迹的概率分布由每一步的状态和动作决定</p>$$P(\tau; \theta) = p(s_0) \prod_{t=0}^{T} \pi_\theta(a_t | s_t) p(s_{t+1} | s_t, a_t)$$<blockquote><p>前面乘$p(s_0)$是考虑随机初始状态的情况</p></blockquote><p>取对数，连乘变成了求和</p>$$\log P(\tau; \theta) = \log p(s_0) + \sum_{t=0}^{T} \left[ \log \pi_\theta(a_t | s_t) + \log p(s_{t+1} | s_t, a_t) \right]$$<p>求导，常数项都没了，可得</p>$$\nabla_\theta \log P(\tau; \theta) = \nabla_\theta \left[ \log p(s_0) + ... \right] = \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t)$$$${\color{red}\nabla_\theta J(\theta)= E_{\tau \sim \pi_\theta} [ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G(\tau)]={\boxed {E_{\tau \sim \pi_\theta} [ \sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G_t]}}}$$<blockquote><p>$G(\tau)$是<strong>完成整条轨迹</strong>的回报，而在时间步</p>$$t$$<p>之前的行为已经发生。</p></blockquote><p><strong>策略梯度定理告诉我们：不需要知道环境模型（即状态转移概率 $p(s'|s, a)$），只需要根据当前策略$\pi_\theta$ 与环境交互得到的样本轨迹$\tau$，就能计算出优化策略所需梯度的无偏估计。</strong></p><blockquote><p>很伟大，为$\nabla_\theta J(\theta)$这个黑箱提供了可计算的方法</p><p>直观理解：如果一个动作 ($a_t$) 之后获得了很高的回报 ($G_t$)，我们就大幅调整参数，使得未来在类似状态 ($s_t$) 下选择这个动作的概率大大增加。反之，如果回报很低，我们就降低选择它的概率。</p></blockquote><p>上面的推导过程是在有限轨迹的任务中，但实际问题中轨迹空间很大，因此需要一个一般形式的策略梯度定理</p><p>用价值函数替代轨迹的回报，因为</p>$$Q^{\pi_\theta}(s,a) = \mathbb{E}_{\pi_\theta} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \mid s_t = s, a_t = a \right]$$$${\boxed {{\color{red} \nabla_\theta J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) \cdot Q^{\pi_\theta}(s,a) \right]}}}$$<ol><li><h2 id=reinforce算法>REINFORCE算法<a hidden class=anchor aria-hidden=true href=#reinforce算法>#</a></h2></li></ol><p><strong>核心思路：基于</strong>**<a href=https://o9vao24145.feishu.cn/docx/Uz2SdlWMxojKUYxbTnrcmid3nrc#share-CA6mdn8x7oKcCLxq3I9czRPSnKb>策略梯度定理</a>****，用蒙特卡洛方法估计策略梯度。**</p><p><strong>进行采样的理由：</strong></p><ul><li>策略梯度定理给出的是期望形式的梯度，需要对所有可能轨迹或所有状态-动作对求期望。</li><li>但在实际环境中，<strong>状态和轨迹空间巨大甚至无限</strong>，无法穷举计算，因此必须通过采样轨迹进行蒙特卡洛估计。</li></ul><blockquote><p><em>蒙特卡洛方法：</em></p><p><em>通过“随机采样”来估计数学量（如期望、积分、概率等）的数值计算方法，多次采样求平均作为期望。</em></p><p>在强化学习中，蒙特卡洛方法通过完整轨迹的回报（return）来估计价值函数或策略梯度，不依赖环境模型或自举（bootstrapping）。</p></blockquote><p><strong>REINFORCE的优点：</strong></p><ul><li>无偏估计（Unbiased）：采样越多，估计越准（大数定律保证），没有模型误差或自举偏差。</li><li>模型无关：不需要知道环境转移概率 $p(s'∣s,a)$，也不需要学习价值函数</li><li>实现简单、概念清晰：<strong>采样轨迹 → 计算算回报 → 计算算梯度 → 更新参数</strong></li><li>在足够采样和小学习率下，能收敛到局部最优（在策略梯度框架内）</li><li>策略是随机的，天然支持探索（对比 ε-greedy 的硬切换）</li></ul><p><strong>重点来了🌟：</strong></p><p>然而，由于使用完整轨迹的总回报$G_t$作为价值估计，且未利用任何函数近似或基线（baseline），即使在长期采样下期望是无偏估计，但单次采样的梯度噪声极大，**REINFORCE 的梯度估计方差极高，**会导致训练过程不稳定、收敛缓慢，尤其在长轨迹或稀疏奖励环境中表现更差。</p><blockquote><p>后面的Actor-Critic、TRPO、PPO基本都是在<strong>解决大方差的问题</strong></p><ul><li>Actor-Critic结构引入Critic 网络作为价值函数的估计以减小方差</li><li>TRPO用KL散度限制每次策略更新的距离，因为即使梯度方向对，一步更新太大也会导致策略“崩溃”——因为新策略下的数据分布变了，旧梯度不再适用</li><li>PPO则直接裁剪梯度，算是对TRPO对工程优化</li></ul></blockquote><h2 id=关于强化学习中期望估计的方法总结>关于强化学习中期望估计的方法总结：<a hidden class=anchor aria-hidden=true href=#关于强化学习中期望估计的方法总结>#</a></h2><p>可以从两个维度分类：</p><ol><li><strong>按是否使用“自举”（Bootstrapping）</strong> → 蒙特卡洛 vs. 时序差分</li><li><strong>按是否使用“函数逼近”</strong> → 表格式 vs. 神经网络/线性逼近</li></ol><p>常见的有下面几种</p><table><thead><tr><th>方法</th><th>全称</th><th>核心思想</th><th>是否自举</th><th>是否无偏</th><th>方差</th><th>样本效率</th><th>适用场景</th></tr></thead><tbody><tr><td><strong>MC</strong></td><td>蒙特卡洛（Monte Carlo）</td><td>用完整轨迹的实际回报估计期望</td><td>❌ 否</td><td>✅ 是</td><td>⬆️ 高</td><td>❌ 低</td><td>Episodic任务，无需模型</td></tr><tr><td><strong>TD</strong></td><td>时序差分（Temporal Difference）</td><td>用一步预测 + 下一状态估计来更新</td><td>✅ 是</td><td>❌ 否（有偏）</td><td>⬇️ 低</td><td>✅ 高</td><td>在线学习，continuing任务</td></tr><tr><td><strong>TD(λ)</strong> / <strong>GAE</strong></td><td>多步时序差分 / 广义优势估计</td><td>MC 与 TD 的折中，加权多步回报</td><td>✅ 是</td><td>❌ 近似无偏（λ→1）</td><td>🟡 中</td><td>✅ 高</td><td>策略梯度、Actor-Critic</td></tr></tbody></table><p>MC的方差大，<a href=https://o9vao24145.feishu.cn/docx/Uz2SdlWMxojKUYxbTnrcmid3nrc#share-VpJQdERyioUmTmxVqNhcYUMmnHc>前面</a>提到了。</p><p>TD的方差小，<a href=https://o9vao24145.feishu.cn/docx/Ulm9d1ICvoXzWExxTWLcO5nZn5c#share-JOlUdVzvqopgDVxxfWEczaw5nCg>是因为估计过程是逐步进行的，两步之间的相关性比较高</a>。<a href=https://o9vao24145.feishu.cn/docx/Ulm9d1ICvoXzWExxTWLcO5nZn5c#share-NbKbdABzFoekDwxArdpcjobrnkc>但在 DQN 中，这种相关性 + 自举 + 函数逼近导致训练不稳定，必须用BufferReplay等技巧去除相关性。</a></p><blockquote><p><strong>TD(λ)</strong> / <strong>GAE还没学，看了Actor-Critic之后再回来分析</strong></p></blockquote><h2 id=致命三角deadly-triad>致命三角Deadly Triad：<a hidden class=anchor aria-hidden=true href=#致命三角deadly-triad>#</a></h2><p>“致命三角”指的是函数逼近、自举和离线策略学习三者结合时可能导致的不收敛问题。</p><blockquote><ul><li>离线策略（Off-Policy）： 用来生成行为（收集数据）的策略（行为策略 <code>β(a|s)</code>）与我们要学习改进的策略（目标策略 <code>π(a|s)</code>）是不同的。</li><li>自举（Bootstrapping）：在价值函数学习中，自举是指使用下一个状态（或动作）的“当前估计值”来计算当前状态（或动作）的目标值（target）。</li><li>函数逼近（Function Approximation)：使用参数化函数（如神经网络）来近似值函数$V(s)$或$Q(s, a)$$或策略函数$$π(a|s)$。多数时候如此因为状态空间太大或连续，无法用表格表示。</li></ul></blockquote><p><strong>函数逼近 + 自举 + 离线策略= 训练不稳定</strong></p><p>这三个因素中的任意一个单独出现，通常都是可管理的。但当它们结合时，会产生一种危险的正反馈循环：</p><ol><li>初始误差： 由于函数逼近，我们的值函数初始就存在误差。</li><li>误差传播与放大： 自举过程会将这些误差从后续状态（s'）传播回当前状态（s）。更糟糕的是，在离策略学习中，我们用于自举的数据（来自行为策略β）可能与目标策略π下的真实数据分布不符，这往往会系统性高估或低估某些值，从而放大这些误差。</li><li>目标漂移： 由于我们的更新目标是基于有误差的估计值，而这个目标本身也在不断移动（因为我们在不断更新网络）。这导致我们像是在追逐一个移动的、不准的靶子。</li><li>发散： 最终，这种误差的传播和放大可能导致值函数的估计彻底失控，变得无穷大（发散）或者收敛到一个完全错误的值。</li></ol><blockquote><p>$Q-learning$ + $Q(\theta)$。</p><ul><li>Q-learning是Off-Policy的，因为收集数据的策略是$ε-greedy$，学习是贪婪策略的Q值</li><li>Q-learning的自举过程引入了估计偏差（因为目标值依赖当前不完美的 Q 估计），在表格法 + 无限采样 + 合适学习率条件下，Q 值可收敛到无偏最优值，即Q-learning本身是可收敛的</li><li>但是引入神经网络对Q进行估计时，如果某个 Q 值被高估，那下一步目标也被高估，错误放大从而导致训练发散</li><li>所以DQN采用目标网络（Target Network），延迟更新目标 Q 值，打断自举导致的正反馈循环，防止 Q 值爆炸性增长。</li></ul></blockquote><blockquote><p>一个简单的比喻：想象你在教一个机器人走路（目标策略π），但为了收集数据，你让另一个醉酒的机器人（行为策略β）去到处晃悠。</p><ul><li>函数逼近： 你用一个简单的模型来总结醉酒机器人的经验。</li><li>自举： 机器人根据这个不完美的模型来做决策（“如果我从这里迈一步，根据模型估计，我可能会摔倒”）。</li><li>离线策略： 你用醉酒机器人的数据来教清醒机器人走路。</li></ul></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/>强化学习</a></li><li><a href=https://nky10.github.io/tags/reinforce/>REINFORCE</a></li><li><a href=https://nky10.github.io/tags/%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86/>策略梯度定理</a></li><li><a href=https://nky10.github.io/tags/%E8%87%B4%E5%91%BD%E4%B8%89%E8%A7%92/>致命三角</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>