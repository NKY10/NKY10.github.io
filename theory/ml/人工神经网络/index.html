<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>人工神经网络 | openset的博客</title><meta name=keywords content="神经网络"><meta name=description content="摘要：推导人工神经网络网络前向传播及反向传播的数学原理，并基于matlab实现全过程。"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="人工神经网络"><meta property="og:description" content="摘要：推导人工神经网络网络前向传播及反向传播的数学原理，并基于matlab实现全过程。"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-09-02T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-02T00:00:00+00:00"><meta property="article:tag" content="神经网络"><meta name=twitter:card content="summary"><meta name=twitter:title content="人工神经网络"><meta name=twitter:description content="摘要：推导人工神经网络网络前向传播及反向传播的数学原理，并基于matlab实现全过程。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"机器学习","item":"https://nky10.github.io/theory/ml/"},{"@type":"ListItem","position":3,"name":"人工神经网络","item":"https://nky10.github.io/theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"人工神经网络","name":"人工神经网络","description":"摘要：推导人工神经网络网络前向传播及反向传播的数学原理，并基于matlab实现全过程。","keywords":["神经网络"],"articleBody":"基本概念 ​\t神经网络，模拟生物神经网络，节点称为神经元。神经网络分层命名，直接接收输入数据的是输入层，输出结果的是输出层，输入层与输出层之间的是隐藏层。\n​\t前向传播：数据从输入层开始，逐层向前传播计算，直到输出层得到输出结果。\n​\t反向传播：将输出层输出的结果与真实值进行比较，得到一个差异（损失函数值），根据这个差异逐层调整网络中的参数，方向与前向传播相反。\n​\t神经网络中的参数：以输入层和第一个隐藏层为例。假设输入数据是一个$m$维向量$x$(即输入层有$m$个神经元)，给定第一个隐藏层有$n$个神经元，层之间数据的传递过程是一个基于*权重$\\omega$和偏置$b$***的线性运算，$\\omega$是一个$n\\times m$维矩阵，$b$是一个$n$维向量，计算过程为$ z=\\omega\\cdot x^T+b$，$z$是该隐藏层神经元的输入值（除了输入层神经元外其余层的神经元均有一个输入和输出值），$\\omega$在图里就是神经元之间的连线。\n​\t整个训练神经网络的过程本质就是不停地在各层前向传播、计算误差并反向传播来得到一组较好的网络参数（各层之间的权重和偏置）。可以理解为\"大力出奇迹\"。\n​\t激活函数：前面提到除了输入层神经元外其余层的神经元均有一个输入$z$和输出值$f(z)$，$f$是该神经元的激活函数，目的是引入非线性关系（如果没有这个激活函数，那么无论有多少隐藏层，最后得到的整个系统输入输出关系本质还是一个线性的）。\n​\t损失函数：分类问题的结果是离散值，平方损失函数如果用于分类问题，分类正确的误差都是一样的，分类错误的误差都是一样，不具有距离的意义，在分类错误的情况下无法判断优化的好坏。并且平方损失误差还和预测结果错误的样本有联系，在分类问题中只关注分类正确的结果。所以 平方损失函数不适用于分类问题。交叉熵损失函数针对只分类正确的预测结果，和分类错误的预测结果没有联系，而分类任务中很少考虑分类错误的结果，主要考虑让正确的分类尽可能大。而在回归中，考虑错误的结果就很必要了，需要让回归的函数尽可能满足所有的样本包括错误的样本，所以交叉熵损失函数不适合回归。\n常用的激活函数：\n​\t关于Sigmoid $$\r\\begin{align}\rs\u0026=\\text{sigmoid}(x)=\\frac{1}{1+\\exp(-x)}\\\\\r\\frac{\\partial s}{\\partial x}\u0026=\\frac{1}{1+\\exp(-x)} \\cdot \\frac{\\exp(-x)}{1+\\exp(-x)}=s \\cdot (1-s)\\\\\r\\end{align}\r$$ ​\t关于Softmax函数及其导数：其中 $$\r\\begin{align}\rs_i\u0026=\\text{softmax}(x_i)=\\frac{\\exp(x_i)}{\\sum_{j=1}^{N}\\exp(x_j)}\\\\\r\\frac{\\partial s_i}{\\partial x_j}\u0026=s_i-s_j^2 \\quad\\quad(i=j)\\\\\r\\frac{\\partial s_i}{\\partial x_j}\u0026=-s_i \\cdot s_j \\quad\\quad(i \\neq j)\r\\end{align}\r$$传播过程 ​\t以如下网络为例分析传播过程。\n​\t$x_i$表示输入层的输入，$\\delta_i $表示隐藏层输入，$\\beta_i$表示$\\delta_i$经过激活函数之后的值，$z_i $表示输出层的输入，$\\hat{y}_i$表示$z_i$经过激活函数之后的值即整个网络最终的输出，用$y_i$表示真实值。$\\omega_1$是输入层与隐藏层之间的权重矩阵，$b_1$是偏置，$\\omega_2$是隐藏层与输出层之间的权重矩阵，$b_2$是偏置，具体对应神经元之间的权重和偏置用$\\omega_{k-ij},b_{k-ij}$表示。激活函数用$f$表示。\n前向传播 ​\t$x = [x_1,x_2,x_3]$，$\\delta=[\\delta_1,\\delta_2,\\delta_3]$\n​\t输入数据后传播一层 $$\r\\delta = x \\cdot \\omega_1 + b_1\r$$ ​\t经过激活函数$f$后 $$\r\\beta = f(\\delta)\r$$ ​\t传播到下一层 $$\rz = \\beta \\cdot \\omega_2 + b_2\r$$ ​\t经过激活函数$f$后 $$\r\\hat{y} = f(z)\r$$计算损失 ​\t以交叉熵损失函数为例进行说明。$y_{ij}$表示第$i$个样本的真实标签的第$j$个类别的值（0或1），$\\hat{y}$表示模型预测的第$i$个样本的第$j$个类别的概率，$N$为样本数量。 $$\rL =- \\frac{1}{N} \\sum_{i}^{N} \\sum_{j}^{\\text{classNum}}y_{ij}\\log(\\hat{y}_{ij})\r$$ ​\t$y$用向量表示时 $$\rL =- \\frac{1}{N} \\sum_{i}^{N} y_{i}\\log(\\hat{y}_{i})\r$$ ​\t调整参数（权重矩阵和偏置）：让损失尽可能快的降低，需要沿着损失函数的其梯度下降，给出学习率$\\eta$作为下降的步长，$\\Delta\\omega$和$\\Delta b$分别是$L$对$\\omega$和$b$的导数。 $$\r\\omega = \\omega - \\eta \\cdot \\Delta\\omega\r$$ $$\rb = \\omega - \\eta \\cdot \\Delta b\r$$调整隐藏层和输出层的参数 ​\t权重参数的变化量 $$\r\\begin{align}\r\\Delta\\omega_2 \u0026= \\frac{\\partial L}{\\partial \\omega_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\omega_2} \\\\\r\\\\\\frac{\\partial L}{\\partial \\hat{y}} \u0026= -\\frac{1}{N}\\sum_{i=1}^{N}\\frac{y_i}{\\hat{y}_i} \\\\\r\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_j} \u0026= -\\frac{1}{N}\\sum_{i=1}^{N}\\frac{y_i}{\\hat{y}_i}\\frac{\\partial \\hat{y}_i}{\\partial z_j} \\\\\r\u0026= -\\frac{1}{N}\\left[\\frac{y_j}{\\hat{y}_j} \\cdot \\left(\\frac{\\hat{y}_j}{z_j} + \\sum_{i \\neq j}^{N}\\frac{\\hat{y}_i}{z_j}\\right)\\right] \\\\\r\u0026= -\\frac{1}{N}\\left[\\left(y_i - \\hat{y}_i y_i\\right) - \\sum_{i \\neq j}^{N}\\hat{y}_i y_i\\right] \\\\\r\u0026= \\frac{1}{N}\\left(\\hat{y}_i - y_i\\right) \\\\\r\\\\\\frac{\\partial z}{\\partial \\omega_2} \u0026= \\beta \\\\\r\\\\\\Delta\\omega_2 \u0026= \\frac{1}{N}\\left(\\hat{y}_i - y_i\\right) \\cdot \\beta \\\\\r\\end{align}\r$$ ​\t同理可得偏置参数的变化量 $$\r\\Delta b_2 = \\frac{1}{N}\\left(\\hat{y}_i - y_i\\right)\r$$调整隐藏层和输出层的参数 $$\r\\begin{align}\r\\Delta\\omega_1 \u0026= \\frac{\\partial L}{\\partial \\omega_1} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\beta} \\cdot \\frac{\\partial \\beta}{\\partial \\delta} \\cdot \\frac{\\partial \\delta}{\\partial \\omega_1} \\\\\r\\frac{\\partial z}{\\partial \\beta} \u0026= \\omega_2 \\\\\r\\frac{\\partial \\beta}{\\partial \\delta} \u0026= \\frac{\\partial f(\\delta)}{\\partial \\delta} = f'(\\delta) = f(\\delta)(1-f(\\delta))\\quad\\text{对于sigmoid} \\\\\r\\frac{\\partial \\delta}{\\partial \\omega_1} \u0026= x \\\\\r\\end{align}\r$$​\t$\\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} $ S 是上一个反向传播的Layer梯度的一部分，从这一计算过程中可以发现，我们在做反向传播进行计算的过程也存在梯度的逐层传播。\n​\t那么这里定义 $$\rgradient_1 = \\frac{1}{N}\\left(\\hat{y}_i - y_i\\right)\r$$ ​\t即 $$\r\\begin{align}\r\\Delta\\omega_2 \u0026= gradient_1 \\cdot \\beta \\\\\r\\Delta b_2 \u0026= gradient_1 \\\\\r\\end{align}\r$$ ​\t那么 $$\r\\begin{align}\r\\Delta\\omega_1 \u0026= gradient_1 \\cdot \\omega_2 \\cdot f'(\\delta) \\cdot x \\\\\r\\Delta b_1 \u0026= gradient_1 \\cdot \\omega_2 \\cdot f'(\\delta) \\\\\r\\end{align}\r$$ ​\t定义 $$\rgradient_2 = gradient_1 \\cdot \\omega_2 \\cdot f'(\\delta)\r$$ ​\t则 $$\r\\begin{align}\r\\Delta\\omega_1 \u0026= gradient_2 \\cdot x \\\\\r\\Delta b_1 \u0026= gradient_2 \\\\\r\\end{align}\r$$ ​\t至此，一轮传播已经完成。\n递推一般情况 ​\t激活函数记为$f$，假设隐藏层$n$与隐藏层$n+1$之间各参数已知，即$gradient_n$、$w_n$等，则反向传播至隐藏层$n-1$与隐藏层$n$时 $$\r\\begin{align}\r\\Delta\\omega_{n-1} \u0026= gradient_n \\cdot \\omega_n \\cdot f'(\\delta) \\cdot x \\\\\r\\Delta b_{n-1} \u0026= gradient_n \\cdot \\omega_n \\cdot f'(\\delta) \\\\\rgradient_{n-1} \u0026= gradient_n \\cdot \\omega_n \\cdot f'(\\delta)\r\\end{align}\r$$ ​\t$x$表示这个Layer的输入，$gradient_n$表示前一个Layer的梯度，$\\omega_n$表示前一个Layer的权重，$\\delta$是当前Layer的输出（通过激活函数之前）。\n优化梯度过程 The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. 该方法通过估计梯度的第一阶矩和第二阶矩来计算不同参数的个体自适应学习率\n结合两种方法实现：AdaGrad很好地处理稀疏梯度，RMSProp很好地处理在线和非平稳环境\nAdam的一些优点是参数更新的大小对梯度的重新缩放是不变的，它的步长大约由步长超参数限定，它不需要一个平稳目标，它与稀疏梯度一起工作，并且它自然地执行一种步长退火形式。\n​\t当我们使用 $m$ 和 $v$ 来更新参数时，目的是为了根据历史梯度信息来调整学习率，以便更好地指导参数的更新方向和步长。具体地说，$m$ 反映了梯度的变化趋势，而 $v$ 反映了梯度的变化幅度。因此，将 $m$ 除以根号 $v$ 可以使得参数更新的步长更加稳定，并且能够在梯度变化幅度较大时缩小步长，在梯度变化幅度较小时放大步长，从而提高了优化算法的性能和收敛速度。\n​\t在更新参数时，我们希望能够在梯度变化较大的方向上小步快跑，在梯度变化较小的方向上大步慢跑。$m/\\sqrt{v}$ 的形式可以很好地实现这一目标，因为它同时考虑了梯度的变化趋势（$m$）和变化幅度（$\\sqrt{v}$），从而使得参数更新更加智能和适应性。\nmatlab代码实现 https://github.com/NKY10/ANN_WITH_MATLAB.git\n","wordCount":"453","inLanguage":"en","datePublished":"2025-09-02T00:00:00Z","dateModified":"2025-09-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">人工神经网络</h1><div class=post-meta><span title='2025-09-02 00:00:00 +0000 UTC'>September 2, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5 aria-label=基本概念>基本概念</a></li><li><a href=#%e4%bc%a0%e6%92%ad%e8%bf%87%e7%a8%8b aria-label=传播过程>传播过程</a><ul><li><a href=#%e5%89%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label=前向传播>前向传播</a></li><li><a href=#%e8%ae%a1%e7%ae%97%e6%8d%9f%e5%a4%b1 aria-label=计算损失>计算损失</a><ul><li><a href=#%e8%b0%83%e6%95%b4%e9%9a%90%e8%97%8f%e5%b1%82%e5%92%8c%e8%be%93%e5%87%ba%e5%b1%82%e7%9a%84%e5%8f%82%e6%95%b0 aria-label=调整隐藏层和输出层的参数>调整隐藏层和输出层的参数</a></li><li><a href=#%e8%b0%83%e6%95%b4%e9%9a%90%e8%97%8f%e5%b1%82%e5%92%8c%e8%be%93%e5%87%ba%e5%b1%82%e7%9a%84%e5%8f%82%e6%95%b0-1 aria-label=调整隐藏层和输出层的参数>调整隐藏层和输出层的参数</a></li><li><a href=#%e9%80%92%e6%8e%a8%e4%b8%80%e8%88%ac%e6%83%85%e5%86%b5 aria-label=递推一般情况>递推一般情况</a></li></ul></li></ul></li><li><a href=#%e4%bc%98%e5%8c%96%e6%a2%af%e5%ba%a6%e8%bf%87%e7%a8%8b aria-label=优化梯度过程>优化梯度过程</a></li><li><a href=#matlab%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0 aria-label=matlab代码实现>matlab代码实现</a></li></ul></div></details></div><div class=post-content><h2 id=基本概念>基本概念<a hidden class=anchor aria-hidden=true href=#基本概念>#</a></h2><p>​ 神经网络，模拟生物神经网络，节点称为<strong>神经元</strong>。神经网络分层命名，直接接收输入数据的是<strong>输入层</strong>，输出结果的是<strong>输出层</strong>，输入层与输出层之间的是<strong>隐藏层</strong>。</p><p>​ <strong>前向传播</strong>：数据从输入层开始，逐层向前传播计算，直到输出层得到输出结果。</p><p>​ <strong>反向传播</strong>：将输出层输出的结果与真实值进行比较，得到一个差异（损失函数值），根据这个差异逐层调整网络中的<em><strong>参数</strong></em>，方向与前向传播相反。</p><p>​ <strong>神经网络中的参数</strong>：以输入层和第一个隐藏层为例。假设输入数据是一个$m$维向量$x$(即输入层有$m$个神经元)，给定第一个隐藏层有$n$个神经元，层之间数据的传递过程是一个基于*<em><em>权重</em>$\omega$<strong>和</strong></em>偏置$b$***的线性运算，$\omega$是一个$n\times m$维矩阵，$b$是一个$n$维向量，计算过程为$ z=\omega\cdot x^T+b$，$z$是该隐藏层神经元的输入值（<em><strong>除了输入层神经元外其余层的神经元均有一个输入和输出值</strong></em>），$\omega$在图里就是神经元之间的连线。</p><p>​ 整个训练神经网络的过程本质就是不停地在各层前向传播、计算误差并反向传播来得到一组较好的网络参数（各层之间的权重和偏置）。可以理解为"大力出奇迹"。</p><p>​ <strong>激活函数</strong>：前面提到除了输入层神经元外其余层的神经元均有一个输入$z$和输出值$f(z)$，$f$是该神经元的激活函数，目的是引入非线性关系（如果没有这个激活函数，那么无论有多少隐藏层，最后得到的整个系统输入输出关系本质还是一个线性的）。</p><p>​ 损失函数：分类问题的结果是离散值，平方损失函数如果用于分类问题，分类正确的误差都是一样的，分类错误的误差都是一样，不具有距离的意义，在分类错误的情况下无法判断优化的好坏。并且平方损失误差还和预测结果错误的样本有联系，在分类问题中只关注分类正确的结果。所以 平方损失函数不适用于分类问题。交叉熵损失函数针对只分类正确的预测结果，和分类错误的预测结果没有联系，而分类任务中很少考虑分类错误的结果，主要考虑让正确的分类尽可能大。而在回归中，考虑错误的结果就很必要了，需要让回归的函数尽可能满足所有的样本包括错误的样本，所以交叉熵损失函数不适合回归。</p><p>常用的激活函数：</p><p>​ 关于Sigmoid</p>$$
\begin{align}
s&=\text{sigmoid}(x)=\frac{1}{1+\exp(-x)}\\
\frac{\partial s}{\partial x}&=\frac{1}{1+\exp(-x)} \cdot \frac{\exp(-x)}{1+\exp(-x)}=s \cdot (1-s)\\
\end{align}
$$<p>​ 关于Softmax函数及其导数：其中</p>$$
\begin{align}
s_i&=\text{softmax}(x_i)=\frac{\exp(x_i)}{\sum_{j=1}^{N}\exp(x_j)}\\
\frac{\partial s_i}{\partial x_j}&=s_i-s_j^2 \quad\quad(i=j)\\
\frac{\partial s_i}{\partial x_j}&=-s_i \cdot s_j \quad\quad(i \neq j)
\end{align}
$$<h2 id=传播过程>传播过程<a hidden class=anchor aria-hidden=true href=#传播过程>#</a></h2><p>​ 以如下网络为例分析传播过程。</p><p>​ $x_i$表示输入层的输入，$\delta_i $表示隐藏层输入，$\beta_i$表示$\delta_i$经过激活函数之后的值，$z_i $表示输出层的输入，$\hat{y}_i$表示$z_i$经过激活函数之后的值即整个网络最终的输出，用$y_i$表示真实值。$\omega_1$是输入层与隐藏层之间的权重矩阵，$b_1$是偏置，$\omega_2$是隐藏层与输出层之间的权重矩阵，$b_2$是偏置，具体对应神经元之间的权重和偏置用$\omega_{k-ij},b_{k-ij}$表示。激活函数用$f$表示。</p><p><img alt=image-20240317160403688 loading=lazy src=../../../theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/images/image-20240317160403688.png></p><h3 id=前向传播>前向传播<a hidden class=anchor aria-hidden=true href=#前向传播>#</a></h3><p>​ $x = [x_1,x_2,x_3]$，$\delta=[\delta_1,\delta_2,\delta_3]$</p><p>​ 输入数据后传播一层</p>$$
\delta = x \cdot \omega_1 + b_1
$$<p>​ 经过激活函数$f$后</p>$$
\beta = f(\delta)
$$<p>​ 传播到下一层</p>$$
z = \beta \cdot \omega_2 + b_2
$$<p>​ 经过激活函数$f$后</p>$$
\hat{y} = f(z)
$$<h3 id=计算损失>计算损失<a hidden class=anchor aria-hidden=true href=#计算损失>#</a></h3><p>​ 以交叉熵损失函数为例进行说明。$y_{ij}$表示第$i$个样本的真实标签的第$j$个类别的值（0或1），$\hat{y}$表示模型预测的第$i$个样本的第$j$个类别的概率，$N$为样本数量。</p>$$
L =- \frac{1}{N} \sum_{i}^{N} \sum_{j}^{\text{classNum}}y_{ij}\log(\hat{y}_{ij})
$$<p>​ $y$用向量表示时</p>$$
L =- \frac{1}{N} \sum_{i}^{N} y_{i}\log(\hat{y}_{i})
$$<p>​ 调整参数（权重矩阵和偏置）：让损失尽可能快的降低，需要沿着损失函数的其梯度下降，给出学习率$\eta$作为下降的步长，$\Delta\omega$和$\Delta b$分别是$L$对$\omega$和$b$的导数。</p>$$
\omega = \omega - \eta \cdot \Delta\omega
$$<p></p>$$
b = \omega - \eta \cdot \Delta b
$$<h4 id=调整隐藏层和输出层的参数>调整隐藏层和输出层的参数<a hidden class=anchor aria-hidden=true href=#调整隐藏层和输出层的参数>#</a></h4><p>​ 权重参数的变化量</p>$$
\begin{align}
\Delta\omega_2 &= \frac{\partial L}{\partial \omega_2} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial \omega_2} \\
\\\frac{\partial L}{\partial \hat{y}} &= -\frac{1}{N}\sum_{i=1}^{N}\frac{y_i}{\hat{y}_i} \\
\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_j} &= -\frac{1}{N}\sum_{i=1}^{N}\frac{y_i}{\hat{y}_i}\frac{\partial \hat{y}_i}{\partial z_j} \\
&= -\frac{1}{N}\left[\frac{y_j}{\hat{y}_j} \cdot \left(\frac{\hat{y}_j}{z_j} + \sum_{i \neq j}^{N}\frac{\hat{y}_i}{z_j}\right)\right] \\
&= -\frac{1}{N}\left[\left(y_i - \hat{y}_i y_i\right) - \sum_{i \neq j}^{N}\hat{y}_i y_i\right] \\
&= \frac{1}{N}\left(\hat{y}_i - y_i\right) \\
\\\frac{\partial z}{\partial \omega_2} &= \beta \\
\\\Delta\omega_2 &= \frac{1}{N}\left(\hat{y}_i - y_i\right) \cdot \beta \\
\end{align}
$$<p>​ 同理可得偏置参数的变化量</p>$$
\Delta b_2 = \frac{1}{N}\left(\hat{y}_i - y_i\right)
$$<h4 id=调整隐藏层和输出层的参数-1>调整隐藏层和输出层的参数<a hidden class=anchor aria-hidden=true href=#调整隐藏层和输出层的参数-1>#</a></h4>$$
\begin{align}
\Delta\omega_1 &= \frac{\partial L}{\partial \omega_1} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial \beta} \cdot \frac{\partial \beta}{\partial \delta} \cdot \frac{\partial \delta}{\partial \omega_1} \\
\frac{\partial z}{\partial \beta} &= \omega_2 \\
\frac{\partial \beta}{\partial \delta} &= \frac{\partial f(\delta)}{\partial \delta} = f'(\delta) = f(\delta)(1-f(\delta))\quad\text{对于sigmoid} \\
\frac{\partial \delta}{\partial \omega_1} &= x \\
\end{align}
$$<p>​ $\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} $ S 是上一个反向传播的Layer梯度的一部分，从这一计算过程中可以发现，我们在做反向传播进行计算的过程也存在梯度的逐层传播。</p><p>​ 那么这里定义</p>$$
gradient_1 = \frac{1}{N}\left(\hat{y}_i - y_i\right)
$$<p>​ 即</p>$$
\begin{align}
\Delta\omega_2 &= gradient_1 \cdot \beta \\
\Delta b_2 &= gradient_1 \\
\end{align}
$$<p>​ 那么</p>$$
\begin{align}
\Delta\omega_1 &= gradient_1 \cdot \omega_2 \cdot f'(\delta) \cdot x \\
\Delta b_1 &= gradient_1 \cdot \omega_2 \cdot f'(\delta) \\
\end{align}
$$<p>​ 定义</p>$$
gradient_2 = gradient_1 \cdot \omega_2 \cdot f'(\delta)
$$<p>​ 则</p>$$
\begin{align}
\Delta\omega_1 &= gradient_2 \cdot x \\
\Delta b_1 &= gradient_2 \\
\end{align}
$$<p>​ 至此，一轮传播已经完成。</p><h4 id=递推一般情况>递推一般情况<a hidden class=anchor aria-hidden=true href=#递推一般情况>#</a></h4><p><img alt=image-20240331233349508 loading=lazy src=../../../theory/ml/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/images/image-20240331233349508.png></p><p>​ 激活函数记为$f$，假设隐藏层$n$与隐藏层$n+1$之间各参数已知，即$gradient_n$、$w_n$等，则反向传播至隐藏层$n-1$与隐藏层$n$时</p>$$
\begin{align}
\Delta\omega_{n-1} &= gradient_n \cdot \omega_n \cdot f'(\delta) \cdot x \\
\Delta b_{n-1} &= gradient_n \cdot \omega_n \cdot f'(\delta) \\
gradient_{n-1} &= gradient_n \cdot \omega_n \cdot f'(\delta)
\end{align}
$$<p>​ $x$表示这个Layer的输入，$gradient_n$表示前一个Layer的梯度，$\omega_n$表示前一个Layer的权重，$\delta$是当前Layer的输出（通过激活函数之前）。</p><h2 id=优化梯度过程>优化梯度过程<a hidden class=anchor aria-hidden=true href=#优化梯度过程>#</a></h2><blockquote><p>The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients. 该方法通过估计梯度的第一阶矩和第二阶矩来计算不同参数的个体自适应学习率</p><p><strong>结合两种方法实现</strong>：AdaGrad很好地处理稀疏梯度，RMSProp很好地处理在线和非平稳环境</p><p><strong>Adam的一些优点</strong>是参数更新的大小对梯度的重新缩放是不变的，它的步长大约由步长超参数限定，它不需要一个平稳目标，它与稀疏梯度一起工作，并且它自然地执行一种步长退火形式。</p></blockquote><p>​ 当我们使用 $m$ 和 $v$ 来更新参数时，目的是为了根据历史梯度信息来调整学习率，以便更好地指导参数的更新方向和步长。具体地说，$m$ 反映了梯度的变化趋势，而 $v$ 反映了梯度的变化幅度。因此，将 $m$ 除以根号 $v$ 可以使得参数更新的步长更加稳定，并且能够在梯度变化幅度较大时缩小步长，在梯度变化幅度较小时放大步长，从而提高了优化算法的性能和收敛速度。</p><p>​ 在更新参数时，我们希望能够在梯度变化较大的方向上小步快跑，在梯度变化较小的方向上大步慢跑。$m/\sqrt{v}$ 的形式可以很好地实现这一目标，因为它同时考虑了梯度的变化趋势（$m$）和变化幅度（$\sqrt{v}$），从而使得参数更新更加智能和适应性。</p><h2 id=matlab代码实现>matlab代码实现<a hidden class=anchor aria-hidden=true href=#matlab代码实现>#</a></h2><p><a href=https://github.com/NKY10/ANN_WITH_MATLAB.git>https://github.com/NKY10/ANN_WITH_MATLAB.git</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>