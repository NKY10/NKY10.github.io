<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>QwenNext结构 | openset的博客</title><meta name=keywords content="模型结构,注意力机制"><meta name=description content="摘要：QwenNext模型结构解读"><meta name=author content><link rel=canonical href=https://nky10.github.io/theory/llm/qwennext%E7%BB%93%E6%9E%84/><link crossorigin=anonymous href=../../../assets/css/stylesheet.08f7d74f0ada0f975d29ae436285b61ed7a719d05f350cb888d00341642995a2.css integrity="sha256-CPfXTwraD5ddKa5DYoW2HtenGdBfNQy4iNADQWQplaI=" rel="preload stylesheet" as=style><link rel=icon href=https://nky10.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://nky10.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://nky10.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://nky10.github.io/apple-touch-icon.png><link rel=mask-icon href=https://nky10.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://nky10.github.io/theory/llm/qwennext%E7%BB%93%E6%9E%84/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams",processRefs:!1},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>.MathJax{outline:0}.MathJax:focus{outline:none}.MathJax_Display{margin:1em 0}.MathJax span{font-size:inherit}</style><meta property="og:url" content="https://nky10.github.io/theory/llm/qwennext%E7%BB%93%E6%9E%84/"><meta property="og:site_name" content="openset的博客"><meta property="og:title" content="QwenNext结构"><meta property="og:description" content="摘要：QwenNext模型结构解读"><meta property="og:locale" content="zh-cn"><meta property="og:type" content="article"><meta property="article:section" content="theory"><meta property="article:published_time" content="2025-09-20T00:00:00+00:00"><meta property="article:modified_time" content="2025-09-20T00:00:00+00:00"><meta property="article:tag" content="模型结构"><meta property="article:tag" content="注意力机制"><meta name=twitter:card content="summary"><meta name=twitter:title content="QwenNext结构"><meta name=twitter:description content="摘要：QwenNext模型结构解读"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"笔记记录","item":"https://nky10.github.io/theory/"},{"@type":"ListItem","position":2,"name":"大语言模型","item":"https://nky10.github.io/theory/llm/"},{"@type":"ListItem","position":3,"name":"QwenNext结构","item":"https://nky10.github.io/theory/llm/qwennext%E7%BB%93%E6%9E%84/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"QwenNext结构","name":"QwenNext结构","description":"摘要：QwenNext模型结构解读","keywords":["模型结构","注意力机制"],"articleBody":" QwenNext模型核心创新点：\n博客：https://mp.weixin.qq.com/s/STsWFuEkaoUa8J8v_uDhag\n代码：https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_next\n高稀疏度 MoE (High-Sparsity MoE): 实现了极低的计算激活比，在保持庞大知识容量的同时，追求极致的推理性能。 混合注意力 (Hybrid Attention): 融合门控增量网络 (Gated DeltaNet) 与门控注意力 (Gated Attention)，高效建模不同距离的上下文依赖。 多词元预测 (MTP): 提升模型性能并为推理加速设计的先进预训练目标。 其他优化: 包括零中心化的 RMSNorm 等，旨在增强训练稳定性。\n混合注意力机制 标准注意力机制 标准注意力机制公式为 $$ Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$ 计算复杂度高：\n\\( QK^\\top \\) 的计算复杂度为 \\( O(n^2 d_k) \\) Softmax 后与 \\( V \\) 相乘为 \\( O(n^2 d_v) \\) 总体复杂度为 \\( O(n^2) \\)，难以适用于当前LLM面临的长文本需求。 低秩现象：\n$QK^T$的秩为$min(n,d)$（$Q$、$K$的维度为$n*d$），当序列长度$n$远大于隐藏层维度$d$时，必然是低秩。 $softmax$输出本质上是一个凸组合（这里是对$ V $中的向量进行加权求和），维度最多为$min(n,d)$，而注意力汇聚在少数位置，那这个凸组合的有效维度远低于$d$ 一组向量的凸组合是其所有向量的加权求和，系数非负。\n凸组合的结果 不会“跑出”这些向量所张成的“凸包”（convex hull）。\n带来的问题：\n表达能力受限：模型无法生成高维、多样化的表示，所有 token 的表示被“压缩”到一个低维子空间中，难以区分语义差异大的内容\n注意力塌缩（Attention Collapse）：多个 query 产生几乎相同的注意力分布，导致不同位置的输出高度相似，丧失位置特异性\nAttention Sink 的根源：低秩结构迫使模型将信息“汇聚”到少数几个“枢纽”token（如开头 token） 这些 token 成为信息中转站，形成 sink\n长上下文建模失效：在长序列中，低秩输出无法承载足够信息，模型难以记住或检索远距离的关键内容（“Lost in the Middle”现象）\n线性注意力机制 一种经典方法是将 softmax 近似为一个可分解的形式，使得注意力计算可以重排为线性复杂度。\n如果能将注意力权重表示为两个向量的外积形式，就可以交换矩阵乘法的顺序，避免显式计算 \\( n \\times n \\) 的注意力矩阵\n一种常见做法是使用**核函数（kernel function）**来替代 softmax1：\n\\[ \\text{LinearAttention}(Q, K, V) = \\frac{\\phi(Q) (\\phi(K)^\\top V)}{\\phi(Q) (\\phi(K)^\\top \\mathbf{1})} \\] 这种线性注意力机制的计算复杂度较低，且无需存储注意力矩阵，内存占用少，适合处理长序列任务。\n但是，由于这种方式是对$softmax$的近似，它会损失一定的精度，且容易受到核函数的影响：\n原始注意力可以精确地为某个关键 token 分配高权重； 线性注意力可能因核函数平滑性或低秩结构，导致权重分布“模糊”，难以聚焦于特定位置； 标准注意力通过 QK 点积天然编码了 token 之间的相对或绝对位置关系（尤其配合ROPE时），而线性注意力在重写过程中，位置信息可能被弱化或平均化，导致模型难以区分“重要但遥远”和“不重要但邻近”的 token。 MiniMax在2025年初发布的MiniMax-01采用了线性注意力Lightning Attention，把注意力计算分成块内和块间两部分，块内用传统注意力计算，块间用线性注意力的核技巧，避免了累积求和操作（cumsum）拖慢速度，实现了百万token的上下文能力2。\nMiniMax-01也采用了混合结构，每7个Lightning Attention后面接一个标准注意力层。\n输出门控注意力机制 Qwen-Next采用输出门控机制3，缓解注意力中的低秩问题。\n在标准的attention输出之后，对每个注意力头应用一个可学习的、查询依赖的 sigmoid 门控\n$$ \\text{Output} = \\sigma(g(Q)) \\odot \\text{Attention}(Q, K, V) $$其中：\n\\( Q, K, V \\) 是查询、键、值； \\( g(Q) \\) 是一个轻量级网络（如线性层）作用于查询 \\(Q\\)； \\( \\sigma \\) 是 sigmoid 激活函数； \\( \\odot \\) 表示逐元素相乘 能解决低秩问题的原因\n引入非线性，打破低秩约束：门控机制通过 非线性变换（sigmoid）对每个位置的注意力输出进行自适应缩放，打破了原始的线性凸组合结构，从而增强了输出的表达能力，缓解了低秩瓶颈。 实现查询依赖的稀疏调制: 门控值由查询 \\(Q\\) 决定，因此不同查询会激活不同的位置，$sigmoid $输出接近 0 或 1，天然具有稀疏性，可抑制无关信息、突出关键内容，这种稀疏调制相当于在注意力输出上施加了一个动态、上下文感知的掩码，有效避免了注意力被“sink”（如重复 token 或早期位置）过度主导。 提升长上下文外推能力: 实验表明，该门控机制显著改善了模型在长上下文任务中的表现。(它减少了对固定位置的过度依赖，使注意力分布更灵活、更聚焦于任务相关区域) Gated DeltaNet 一种新型线性注意力机制，它在架构中发挥了关键性的计算效率优化与长序列建模能力增强作用。\nQwen-Next Qwen3-Next 共 48 层，分为 12 组（每组 4 层）：\n每组的前 3 层：使用 Gated DeltaNet 每组的第 4 层：回归输出门控注意力机制 大部分层用线性注意力，降低整体计算开销；每隔 3 层插入一次 Full Attention，恢复全局交互能力\n未完待续...\nTransformers are RNNs: Fast Autoregressive Transformers with Linear Attention ↩︎\nMiniMax-01: Scaling Foundation Models with Lightning Attention ↩︎\nGated Attention for Large Language Models Non-linearity, Sparsity, and Attention-Sink-Free ↩︎\n","wordCount":"225","inLanguage":"en","datePublished":"2025-09-20T00:00:00Z","dateModified":"2025-09-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://nky10.github.io/theory/llm/qwennext%E7%BB%93%E6%9E%84/"},"publisher":{"@type":"Organization","name":"openset的博客","logo":{"@type":"ImageObject","url":"https://nky10.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nky10.github.io/ accesskey=h title="openset的博客 (Alt + H)">openset的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nky10.github.io/ title=主页><span>主页</span></a></li><li><a href=https://nky10.github.io/theory/ title=理论学习><span>理论学习</span></a></li><li><a href=https://nky10.github.io/explore/ title=实践总结><span>实践总结</span></a></li><li><a href=https://nky10.github.io/anything/ title=随笔><span>随笔</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">QwenNext结构</h1><div class=post-meta><span title='2025-09-20 00:00:00 +0000 UTC'>September 20, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e6%b7%b7%e5%90%88%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=混合注意力机制>混合注意力机制</a><ul><li><a href=#%e6%a0%87%e5%87%86%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=标准注意力机制>标准注意力机制</a></li><li><a href=#%e7%ba%bf%e6%80%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=线性注意力机制>线性注意力机制</a></li><li><a href=#%e8%be%93%e5%87%ba%e9%97%a8%e6%8e%a7%e6%b3%a8%e6%84%8f%e5%8a%9b%e6%9c%ba%e5%88%b6 aria-label=输出门控注意力机制>输出门控注意力机制</a></li><li><a href=#gated-deltanet aria-label="Gated DeltaNet">Gated DeltaNet</a></li><li><a href=#qwen-next aria-label=Qwen-Next>Qwen-Next</a></li></ul></li></ul></div></details></div><div class=post-content><blockquote><p>QwenNext模型核心创新点：</p><p>博客：https://mp.weixin.qq.com/s/STsWFuEkaoUa8J8v_uDhag</p><p>代码：https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_next</p><p>高稀疏度 MoE (High-Sparsity MoE): 实现了极低的计算激活比，在保持庞大知识容量的同时，追求极致的推理性能。
混合注意力 (Hybrid Attention): 融合门控增量网络 (Gated DeltaNet) 与门控注意力 (Gated Attention)，高效建模不同距离的上下文依赖。
多词元预测 (MTP): 提升模型性能并为推理加速设计的先进预训练目标。
其他优化: 包括零中心化的 RMSNorm 等，旨在增强训练稳定性。</p></blockquote><h2 id=混合注意力机制>混合注意力机制<a hidden class=anchor aria-hidden=true href=#混合注意力机制>#</a></h2><h3 id=标准注意力机制>标准注意力机制<a hidden class=anchor aria-hidden=true href=#标准注意力机制>#</a></h3><p>标准注意力机制公式为</p>$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$<p><strong>计算复杂度高</strong>：</p><ul><li>\( QK^\top \) 的计算复杂度为 \( O(n^2 d_k) \)</li><li>Softmax 后与 \( V \) 相乘为 \( O(n^2 d_v) \)</li><li>总体复杂度为 \( O(n^2) \)，难以适用于当前LLM面临的长文本需求。</li></ul><p><strong>低秩现象</strong>：</p><ul><li>$QK^T$的秩为$min(n,d)$（$Q$、$K$的维度为$n*d$），当序列长度$n$远大于隐藏层维度$d$时，必然是低秩。</li><li>$softmax$输出本质上是一个凸组合（这里是对$ V $中的向量进行加权求和），维度最多为$min(n,d)$，而注意力汇聚在少数位置，那这个凸组合的有效维度远低于$d$</li></ul><blockquote><p>一组向量的凸组合是其所有向量的加权求和，系数非负。</p><p>凸组合的结果 不会“跑出”这些向量所张成的“凸包”（convex hull）。</p></blockquote><p>带来的问题：</p><ul><li><p>表达能力受限：模型无法生成高维、多样化的表示，所有 token 的表示被“压缩”到一个低维子空间中，难以区分语义差异大的内容</p></li><li><p>注意力塌缩（Attention Collapse）：多个 query 产生几乎相同的注意力分布，导致不同位置的输出高度相似，丧失位置特异性</p></li><li><p>Attention Sink 的根源：低秩结构迫使模型将信息“汇聚”到少数几个“枢纽”token（如开头 token）
这些 token 成为信息中转站，形成 sink</p></li><li><p>长上下文建模失效：在长序列中，低秩输出无法承载足够信息，模型难以记住或检索远距离的关键内容（“Lost in the Middle”现象）</p></li></ul><h3 id=线性注意力机制>线性注意力机制<a hidden class=anchor aria-hidden=true href=#线性注意力机制>#</a></h3><p>一种经典方法是将 softmax 近似为一个可分解的形式，使得注意力计算可以重排为线性复杂度。</p><blockquote><p>如果能将注意力权重表示为两个向量的外积形式，就可以交换矩阵乘法的顺序，避免显式计算 \( n \times n \) 的注意力矩阵</p></blockquote><p>一种常见做法是使用**核函数（kernel function）**来替代 softmax<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>：</p>\[
\text{LinearAttention}(Q, K, V) = \frac{\phi(Q) (\phi(K)^\top V)}{\phi(Q) (\phi(K)^\top \mathbf{1})}
\]<p>这种线性注意力机制的计算复杂度较低，且无需存储注意力矩阵，内存占用少，适合处理长序列任务。</p><p>但是，由于这种方式是对$softmax$的近似，它会损失一定的精度，且容易受到核函数的影响：</p><ul><li>原始注意力可以精确地为某个关键 token 分配高权重；</li><li>线性注意力可能因核函数平滑性或低秩结构，导致权重分布“模糊”，难以聚焦于特定位置；</li><li>标准注意力通过 QK 点积天然编码了 token 之间的相对或绝对位置关系（尤其配合ROPE时），而线性注意力在重写过程中，位置信息可能被弱化或平均化，导致模型难以区分“重要但遥远”和“不重要但邻近”的 token。</li></ul><blockquote><p>MiniMax在2025年初发布的MiniMax-01采用了线性注意力Lightning Attention，把注意力计算分成块内和块间两部分，块内用传统注意力计算，块间用线性注意力的核技巧，避免了累积求和操作（cumsum）拖慢速度，实现了百万token的上下文能力<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>。</p><p>MiniMax-01也采用了混合结构，每7个Lightning Attention后面接一个标准注意力层。</p></blockquote><h3 id=输出门控注意力机制>输出门控注意力机制<a hidden class=anchor aria-hidden=true href=#输出门控注意力机制>#</a></h3><p>Qwen-Next采用输出门控机制<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>，缓解注意力中的低秩问题。</p><p>在标准的attention输出之后，对每个注意力头应用一个可学习的、查询依赖的 sigmoid 门控</p>$$
\text{Output} = \sigma(g(Q)) \odot \text{Attention}(Q, K, V)
$$<p>其中：</p><ul><li>\( Q, K, V \) 是查询、键、值；</li><li>\( g(Q) \) 是一个轻量级网络（如线性层）作用于查询 \(Q\)；</li><li>\( \sigma \) 是 sigmoid 激活函数；</li><li>\( \odot \) 表示逐元素相乘</li></ul><p><strong>能解决低秩问题的原因</strong></p><ol><li><strong>引入非线性，打破低秩约束</strong>：门控机制通过 <strong>非线性变换</strong>（sigmoid）对每个位置的注意力输出进行<strong>自适应缩放</strong>，打破了原始的线性凸组合结构，从而<strong>增强了输出的表达能力</strong>，缓解了低秩瓶颈。</li><li><strong>实现查询依赖的稀疏调制</strong>: 门控值由查询 \(Q\) 决定，因此不同查询会激活不同的位置，$sigmoid $输出接近 0 或 1，天然具有<strong>稀疏性</strong>，可抑制无关信息、突出关键内容，这种稀疏调制相当于在注意力输出上施加了一个<strong>动态、上下文感知的掩码</strong>，有效避免了注意力被“sink”（如重复 token 或早期位置）过度主导。</li><li>提升长上下文外推能力: 实验表明，该门控机制显著改善了模型在<strong>长上下文任务</strong>中的表现。(它减少了对固定位置的过度依赖，使注意力分布更灵活、更聚焦于任务相关区域)</li></ol><h3 id=gated-deltanet>Gated DeltaNet<a hidden class=anchor aria-hidden=true href=#gated-deltanet>#</a></h3><p>一种新型线性注意力机制，它在架构中发挥了关键性的计算效率优化与长序列建模能力增强作用。</p><h3 id=qwen-next>Qwen-Next<a hidden class=anchor aria-hidden=true href=#qwen-next>#</a></h3><p>Qwen3-Next 共 48 层，分为 12 组（每组 4 层）：</p><ul><li>每组的前 3 层：使用 Gated DeltaNet</li><li>每组的第 4 层：回归输出门控注意力机制</li></ul><p>大部分层用线性注意力，降低整体计算开销；每隔 3 层插入一次 Full Attention，恢复全局交互能力</p><hr><p>未完待续...</p><blockquote></blockquote><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href="https://proceedings.mlr.press/v119/katharopoulos20a.html?ref=mackenziemorehead.com">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://arxiv.org/pdf/2501.08313>MiniMax-01: Scaling Foundation Models with Lightning Attention</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://arxiv.org/abs/2505.06708>Gated Attention for Large Language Models Non-linearity, Sparsity, and Attention-Sink-Free</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://nky10.github.io/tags/%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84/>模型结构</a></li><li><a href=https://nky10.github.io/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/>注意力机制</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://nky10.github.io/>openset的博客</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>