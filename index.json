[{"content":"核心思想 REINFORCE是策略梯度定理最直接的应用，通过蒙特卡洛方法采样轨迹来计算整个轨迹的回报。但这存在着明显的缺点：\n高方差： $G_t$ 依赖于从当前状态开始一系列的动作和状态转移，随机性很大。不同的回合，即使状态和动作相同，$G_t$也可能相差巨大。高方差会导致训练不稳定，收敛缓慢。 效率低：必须等回合结束， 无法进行单步更新，学习效率低。 Actor-Critic框架通过引入一个评论家Critic来解决了REINFORCE的痛点\nActor是策略函数$\\pi(a|s;\\theta)$，由参数的影响下根据环境做出动作 Critic是价值函数（可以是Q也可以是V，通常用V），根据Actor做出的动作的结果$(s, a, r, s')$来更新对状态价值的判断 Actor根据这个反馈来更新自己的策略，决定未来是更倾向于还是更避免做这个动作 反馈是用$$\\text{TD-error}$$，衡量的是当前Critic预测 $V(s)$ 和更优的估计 $r + γ * V(s')$ 之间的差距, 相当于$V(s)$自举更新，但是用真实的奖励不断进行校正了. $\\delta_t = r + \\gamma V(s') - V(s) $ 为什么Critic通常使用$V(s)$而不是$Q(s,a)$？\n策略梯度定理中提到引入基线$V(s)$可以降低方差，即用$Q(s,a)-V(s)$\n这就是优势函数$A(s,a)=Q(s,a)-V(s)$\n而在策略$\\pi$下，$Q(s,a)=E[r+\\gamma V(s')]$，因此在单次采样下，可以用$\\delta_t$近似估计$A(s,a)$\n所以只学习$V(s)$，就能近似计算出优势函数，不用专门学习更复杂的$Q(s,a)$\nAC具体流程 Actor在状态$s$做出动作$a$，并根据环境反馈得到奖励$r$和新状态$s'$ Critic根据当前状态$s$和动作$a$，做出自己对这一步的判断，即预测当前状态的价值$V(s)$ Critic根据新状态$s'$和奖励$r$，预测新状态的价值$V(s')$ 计算TD-error：$$\\delta_t = r + \\gamma * V(s') - V(s)$$ 更新Critic的参数，使得预测的价值更接近真实的奖励加上未来的价值：$$V(s) \\leftarrow V(s) + \\alpha * \\delta_t$$ 更新Actor的参数，使得在当前状态下，选择这个动作的概率增加：$$\\theta \\leftarrow \\theta + \\beta * \\delta_t * \\nabla_a \\log \\pi(a|s;\\theta)$$ 重复1-6，直到收敛 Asynchronous Advantage Actor-Critic (A3C) 证明了异步分布式训练和无经验回放算法的可行性\nA3C（异步）的工作流程：\n创建多个环境： 同样运行N个环境副本。\n创建多个线程： 每个worker在一个独立的CPU线程上运行。（这是关键！）\n异步采样： 每个worker拥有主网络参数的本地副本。它们不同步地、独立地从主网络拷贝参数，然后用自己的环境交互若干步。\n异步计算更新： 每个worker根据自己的经验独立地计算梯度。\n异步更新： 计算完成后，每个worker立即将自己的梯度传回给主网络。主网络在不加锁的情况下，持续地用收到的梯度更新自己的参数。\nAdvantage Actor-Critic (A2C) 在AC算法中，我们用TD-error估计优势函数$A(s,a)$，即\n$$\\delta_t = r + γ * V(s') - V(s) \\approx A(s, a)$$虽然是低方差的，但是属于有偏估计。\nA2C采用**n步回报（n-step return）**估计：\n$$A(s,a)\\approx G_t^{(n)}-V(s)$$其中$G_t^{(n)}$是n步回报，$G_t^{(n)}=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\cdots+\\gamma^{n-1} r_{t+n-1}+\\gamma^n V(s_{t+n})$\n当n=1时，就是AC，低方差有偏 当n=∞时，就是MC，高方差无偏 n处于中间，就在MC和AC之间，方差和偏置都在平衡 实际实现中，A2C通常泛指那些使用多个并行环境并同步更新的AC算法。\nA2C的优势：\n降低方差： 通过并行采样，多个worker在不同状态探索，收集到的经验是解相关（decorrelated） 的。将这些经验一起求平均更新，相当于大大降低了梯度的方差。这比在单个环境中顺序采样的方差要小得多。\n更稳定： 同步更新避免了不同步的参数带来的混乱，训练过程更稳定。\n效率高： 在GPU上，对一批并行数据做前向和反向传播远比串行处理一个个环境高效。\n","permalink":"https://nky10.github.io/theory/rl/actor-critic/","summary":"摘要：演员评论家框架的分析","title":"Actor-Critic"},{"content":"1. 引言 基于价值函数的方法（如 Q-learning、DQN）通过估计动作价值函数 $Q(s,a)$ ，并选择使 Q 值最大的动作来进行决策。\n由于需要计算 $arg\\max_{a}Q(s,a)$ ，这类方法天然适用于离散动作空间。虽然 DQN 能处理连续状态空间，但其动作空间必须是离散且有限的； 策略是通过Q间接得到结果的，不够灵活； 探索依赖$\\epsilon -greedy$, 需要一些方法去平衡探索过程，容易限制性能（2. gym-cartpole验证过）。 尽管存在一些扩展方法（如 DDPG、QR-DQN）尝试将价值函数应用于连续动作空间，但在高维或复杂动作空间（如大型语言模型生成 token 的分布）中，这类方法通常不实用\n这时候我们考虑用神经网络对策略函数进行建模，通过梯度更新的方法学习策略网络\n这样的好处在于：\n$\\theta$策略本身就是随机的，探索过程更优 它能够解决连续动作空间和状态空间的问题 2. 核心思想 用一个神经网络来表示策略，参数为$\\theta$\n$$\\pi_\\theta(a|s)=P(a|s;\\theta)$$ $\\pi_\\theta(a|s)$： 策略，由参数 $\\theta$ 控制（比如神经网络的权重）。 $\\tau=(s_0,a_0,r_1,s_1,a_1,r2,...)$: 一条轨迹，代表从开始到结束的一个完整交互序列 $G(\\tau)$: 这个序列的总回报 $P(\\tau;\\theta)$: 策略$\\pi_\\theta$下完成轨迹$\\tau$的概率 我们的目标是找到最优$\\theta^*$,使得智能体与环境交互后的总回报最大, 即\n$$J(\\theta)=E_{\\tau～\\pi_\\theta}[G(\\tau)]=\\sum_\\tau P(\\tau;\\theta)·G(\\tau)$$ $J(\\theta)$是所有可能轨迹的回报，按其发生的概率进行加权平均。一个“好”的策略会让高回报的轨迹出现概率高，从而拉高这个平均值。\n**问题求解：**想要$J(\\theta)$最大，沿着梯度上升的方向更新参数 $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$\n3. 策略梯度定理（Policy Gradient Theorem）推导 对$J(\\theta)$进行求导得 $\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\left[ \\sum_{\\tau} P(\\tau; \\theta) G(\\tau) \\right]$\n式字中$G(\\tau)$由环境决定，为常量，与$\\theta$无关，因此重点在于如何求轨迹概率分布$P(\\tau;\\theta)$的导数。\n计算一个概率分布的梯度：\n取log后求导\n$$\\nabla_\\theta \\log P(\\tau; \\theta) = \\frac{\\nabla_\\theta P(\\tau; \\theta)}{P(\\tau; \\theta)}$$, 因此\n$$\\nabla_\\theta P(\\tau; \\theta) = P(\\tau; \\theta) \\nabla_\\theta \\log P(\\tau; \\theta)$$ 求导\n$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\sum_{\\tau} P(\\tau; \\theta) G(\\tau) = \\sum_{\\tau}\\nabla_\\theta P(\\tau; \\theta) G(\\tau) =\\sum_{\\tau}\\nabla_\\theta P(\\tau; \\theta) \\nabla_\\theta \\log P(\\tau; \\theta)G(\\tau)$$得到中间结果\n$${\\color{blue} \\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log P(\\tau; \\theta) \\cdot G(\\tau) \\right] \\quad }$$轨迹的概率分布由每一步的状态和动作决定\n$$P(\\tau; \\theta) = p(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t | s_t) p(s_{t+1} | s_t, a_t)$$ 前面乘$p(s_0)$是考虑随机初始状态的情况\n取对数，连乘变成了求和\n$$\\log P(\\tau; \\theta) = \\log p(s_0) + \\sum_{t=0}^{T} \\left[ \\log \\pi_\\theta(a_t | s_t) + \\log p(s_{t+1} | s_t, a_t) \\right]$$求导，常数项都没了，可得\n$$\\nabla_\\theta \\log P(\\tau; \\theta) = \\nabla_\\theta \\left[ \\log p(s_0) + ... \\right] = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)$$$${\\color{red}\\nabla_\\theta J(\\theta)= E_{\\tau \\sim \\pi_\\theta} [ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G(\\tau)]={\\boxed {E_{\\tau \\sim \\pi_\\theta} [ \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t]}}}$$ $G(\\tau)$是完成整条轨迹的回报，而在时间步\n$$t$$之前的行为已经发生。\n策略梯度定理告诉我们：不需要知道环境模型（即状态转移概率 $p(s'|s, a)$），只需要根据当前策略$\\pi_\\theta$ 与环境交互得到的样本轨迹$\\tau$，就能计算出优化策略所需梯度的无偏估计。\n很伟大，为$\\nabla_\\theta J(\\theta)$这个黑箱提供了可计算的方法\n直观理解：如果一个动作 ($a_t$) 之后获得了很高的回报 ($G_t$)，我们就大幅调整参数，使得未来在类似状态 ($s_t$) 下选择这个动作的概率大大增加。反之，如果回报很低，我们就降低选择它的概率。\n上面的推导过程是在有限轨迹的任务中，但实际问题中轨迹空间很大，因此需要一个一般形式的策略梯度定理\n用价值函数替代轨迹的回报，因为\n$$Q^{\\pi_\\theta}(s,a) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \\mid s_t = s, a_t = a \\right]$$$${\\boxed {{\\color{red} \\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim d^{\\pi_\\theta}, a \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s,a) \\right]}}}$$REINFORCE算法 核心思路：基于**策略梯度定理****，用蒙特卡洛方法估计策略梯度。**\n进行采样的理由：\n策略梯度定理给出的是期望形式的梯度，需要对所有可能轨迹或所有状态-动作对求期望。 但在实际环境中，状态和轨迹空间巨大甚至无限，无法穷举计算，因此必须通过采样轨迹进行蒙特卡洛估计。 蒙特卡洛方法：\n通过“随机采样”来估计数学量（如期望、积分、概率等）的数值计算方法，多次采样求平均作为期望。\n在强化学习中，蒙特卡洛方法通过完整轨迹的回报（return）来估计价值函数或策略梯度，不依赖环境模型或自举（bootstrapping）。\nREINFORCE的优点：\n无偏估计（Unbiased）：采样越多，估计越准（大数定律保证），没有模型误差或自举偏差。 模型无关：不需要知道环境转移概率 $p(s'∣s,a)$，也不需要学习价值函数 实现简单、概念清晰：采样轨迹 → 计算算回报 → 计算算梯度 → 更新参数 在足够采样和小学习率下，能收敛到局部最优（在策略梯度框架内） 策略是随机的，天然支持探索（对比 ε-greedy 的硬切换） 重点来了🌟：\n然而，由于使用完整轨迹的总回报$G_t$作为价值估计，且未利用任何函数近似或基线（baseline），即使在长期采样下期望是无偏估计，但单次采样的梯度噪声极大，梯度估计方差极高，这会导致训练过程不稳定、收敛缓慢，尤其在长轨迹或稀疏奖励环境中表现更差。 Z\n后面的Actor-Critic、TRPO、PPO基本都是在解决大方差的问题\nActor-Critic结构引入Critic 网络作为价值函数的估计以减小方差 TRPO用KL散度限制每次策略更新的距离，因为即使梯度方向对，一步更新太大也会导致策略“崩溃”——因为新策略下的数据分布变了，旧梯度不再适用 PPO则直接裁剪梯度，算是对TRPO对工程优化 引入基线降低方差\n$$∇J(θ) ∝ Eπ [∇θ log π(a|s) * (Qπ(s, a) - b(s))]$$ 基线 $b(s)$ 不依赖于动作 $a$，这个期望值就保持不变，但方差可以显著降低。\n能最小化方差的最优基线是$b(s)=V_\\pi(s)$即状态价值函数，因为它代表了在状态$s$下遵循策略$\\pi$所能获得的评价期望回报\n原论文提到的这个基线，后来的优势函数就是这个差\n关于强化学习中期望估计的方法总结： 可以从两个维度分类：\n按是否使用“自举”（Bootstrapping） → 蒙特卡洛 vs. 时序差分 按是否使用“函数逼近” → 表格式 vs. 神经网络/线性逼近 常见的有下面几种\n方法 全称 核心思想 是否自举 是否无偏 方差 样本效率 适用场景 MC 蒙特卡洛（Monte Carlo） 用完整轨迹的实际回报估计期望 ❌ 否 ✅ 是 ⬆️ 高 ❌ 低 Episodic任务，无需模型 TD 时序差分（Temporal Difference） 用一步预测 + 下一状态估计来更新 ✅ 是 ❌ 否（有偏） ⬇️ 低 ✅ 高 在线学习，continuing任务 TD(λ) / GAE 多步时序差分 / 广义优势估计 MC 与 TD 的折中，加权多步回报 ✅ 是 ❌ 近似无偏（λ→1） 🟡 中 ✅ 高 策略梯度、Actor-Critic MC的方差大，前面提到了。\nTD的方差小，是因为估计过程是逐步进行的，两步之间的相关性比较高。但在 DQN 中，这种相关性 + 自举 + 函数逼近导致训练不稳定，必须用BufferReplay等技巧去除相关性。\nTD(λ) / GAE还没学，看了Actor-Critic之后再回来分析\n致命三角Deadly Triad： “致命三角”指的是函数逼近、自举和离线策略学习三者结合时可能导致的不收敛问题。\n离线策略（Off-Policy）： 用来生成行为（收集数据）的策略（行为策略 β(a|s)）与我们要学习改进的策略（目标策略 π(a|s)）是不同的。 自举（Bootstrapping）：在价值函数学习中，自举是指使用下一个状态（或动作）的“当前估计值”来计算当前状态（或动作）的目标值（target）。 函数逼近（Function Approximation)：使用参数化函数（如神经网络）来近似值函数$V(s)$或$Q(s, a)$$或策略函数$$π(a|s)$。多数时候如此因为状态空间太大或连续，无法用表格表示。 函数逼近 + 自举 + 离线策略= 训练不稳定\n这三个因素中的任意一个单独出现，通常都是可管理的。但当它们结合时，会产生一种危险的正反馈循环：\n初始误差： 由于函数逼近，我们的值函数初始就存在误差。 误差传播与放大： 自举过程会将这些误差从后续状态（s'）传播回当前状态（s）。更糟糕的是，在离策略学习中，我们用于自举的数据（来自行为策略β）可能与目标策略π下的真实数据分布不符，这往往会系统性高估或低估某些值，从而放大这些误差。 目标漂移： 由于我们的更新目标是基于有误差的估计值，而这个目标本身也在不断移动（因为我们在不断更新网络）。这导致我们像是在追逐一个移动的、不准的靶子。 发散： 最终，这种误差的传播和放大可能导致值函数的估计彻底失控，变得无穷大（发散）或者收敛到一个完全错误的值。 $Q-learning$ + $Q(\\theta)$。\nQ-learning是Off-Policy的，因为收集数据的策略是$ε-greedy$，学习是贪婪策略的Q值 Q-learning的自举过程引入了估计偏差（因为目标值依赖当前不完美的 Q 估计），在表格法 + 无限采样 + 合适学习率条件下，Q 值可收敛到无偏最优值，即Q-learning本身是可收敛的 但是引入神经网络对Q进行估计时，如果某个 Q 值被高估，那下一步目标也被高估，错误放大从而导致训练发散 所以DQN采用目标网络（Target Network），延迟更新目标 Q 值，打断自举导致的正反馈循环，防止 Q 值爆炸性增长。 一个简单的比喻：想象你在教一个机器人走路（目标策略π），但为了收集数据，你让另一个醉酒的机器人（行为策略β）去到处晃悠。\n函数逼近： 你用一个简单的模型来总结醉酒机器人的经验。 自举： 机器人根据这个不完美的模型来做决策（“如果我从这里迈一步，根据模型估计，我可能会摔倒”）。 离线策略： 你用醉酒机器人的数据来教清醒机器人走路。 ","permalink":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95/","summary":"摘要：推导策略梯度定理，介绍REINFORCE的基本原理，分析致命三角问题。","title":"基于策略梯度的方法"},{"content":"1. 开发过程 1.1 dify-plugin-daemon dify插件提供了脚手架，可以一站式配置\nhttps://github.com/langgenius/dify-plugin-daemon\n在releases中选择与插件开发平台相符的工具，本文以win10为例。\n1.2 新建项目 在终端中执行，初始化项目\n1.\\dify-plugin-windows-amd64.exe plugin init 然后配置项目基本信息\n后续内容根据需求选择，看到这里时插件项目创建成功。\n1.3 配置环境 用conda或者uv创建虚拟环境，然后安装依赖。\n1cd say_hello 2pip install -r requirements.txt 这里pip安装的是dify插件环境自身的依赖，而不是功能。具体功能的第三方库在自己开发的时候需要手动安装，然后\n==把它们手动写入requirements.txt==\n否则，debug模式下插件可以用，但是打包之后上传用不了。因为dify会根据这个文件为这个插件创建一个独立的环境来执行，没有库的名称版本，安装就不会成功。\n将.env.example重命名为.env，然后修改debug用的key和ip、端口。key在插件页面查看\nINSTALL_METHOD=remote\rREMOTE_INSTALL_URL=localhost:5003\t# 根据dify的地址确定\rREMOTE_INSTALL_KEY=xxxxxxxxxxxxxx 1.4 编写tool，配置参数 根据需求编写tool/say_hello.py的代码\n1class SayHelloTool(Tool): 2 def _invoke(self, tool_parameters: dict[str, Any]) -\u0026gt; Generator[ToolInvokeMessage]: 3 4 say = tool_parameters.get(\u0026#34;say\u0026#34;,\u0026#34;default\u0026#34;) 5 hello = tool_parameters.get(\u0026#34;hello\u0026#34;,\u0026#34;default2\u0026#34;) 6 7 yield self.create_json_message({ 8 \u0026#34;result\u0026#34;: say+hello 9 }) 这里从tool_parameters中获取了say和hello两个变量，需要在say_hell0.yaml中进行配置，确保dify中可以正常显示\n1parameters: 2 - name: say 3 type: string 4 required: true 5 label: 6 en_US: first string 7 zh_Hans: 第一个字符串 8 pt_BR: first string 9 human_description: 10 en_US: sayhello 11 zh_Hans: sayhello 12 pt_BR: sayhello 13 llm_description: sayhello 14 form: llm 15 - name: hello 16 type: string 17 required: true 18 label: 19 en_US: second string 20 zh_Hans: 第二个字符串 21 pt_BR: second string 22 human_description: 23 en_US: sayhello 24 zh_Hans: sayhello 25 pt_BR: sayhello 26 llm_description: sayhello 27 form: llm 1.5 debug模式下安装 启动main.py，会显示如下内容\n1{\u0026#34;event\u0026#34;: \u0026#34;log\u0026#34;, \u0026#34;data\u0026#34;: {\u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Installed tool: say_hello\u0026#34;, \u0026#34;timestamp\u0026#34;: 1749115119.7156868}} 在dify的插件中可以看到\n2. 插件打包安装 2.1 打包插件 1.\\dify-plugin-windows-amd64.exe plugin package [项目路径] 2# 如 3.\\dify-plugin-windows-amd64.exe plugin package .\\say_hello 此时会得到一个say_hello.difypkg文件，完成开发。\n2.2 插件安装 点击 插件--\u0026gt;本地插件，选择刚刚打包好的say_hello.dify文件，即可完成安装。\n注意，这个步骤需要在联网操作下完成，因为dify会根据requirements.txt为插件创建一个单独的环境，这个过程会下载库。如果需要纯内网环境的安装方式，见第3章。\n3. 内网环境安装 在一些项目的隔离环境中无法联网，需要将插件及其库全部打包好后再上传到内网环境的dify中。\ngithub有一个打包离线版本的脚本，链接是https://github.com/langgenius/dify-official-plugins，使用方法见该项目的readme。\n这个项目目前只能在linux环境下使用。\n最新版本可以windows下打包\n打包插件的时候需要注意Python的版本与Dify是否一致，我的Dify是基于python3.12，如果使用python3.11，会报错。\n插件更新 标准流程是重跑上面的过程\n但是，在插件代码中没有新增依赖时可以直接修改源码实现更新。\n在volumes\\plugin_daemon\\cwd文件夹（默认在dify-main/docker中），找到需要修改代码的插件，打开后修改，然后重启docker-plugin_daemon-1\n","permalink":"https://nky10.github.io/explore/skills/dify%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%E5%8F%8A%E7%A6%BB%E7%BA%BF%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/","summary":"摘要：dify插件开发及离线环境安装记录。","title":"dify插件开发及离线环境安装"},{"content":"1. RAG 基本概念 RAG 是 Retrieval-Augmented Generation 的缩写，即检索增强生成。 基本的RAG有三个阶段：\ngraph TD A[用户输入] --\u0026gt; B[检索] B --\u0026gt; C[生成] ","permalink":"https://nky10.github.io/theory/llm/rag-%E5%85%AB%E8%82%A1%E6%96%87/","summary":"摘要：RAG知识整理","title":"RAG八股文"},{"content":"1. 简化模型分析：无策略下的状态价值函数 在引入策略和动作之前，先考虑一个简化的随机游走模型（即不涉及策略 $\\pi$ 和动作 a），各状态按概率转移，目标是分析状态的价值。\n基本定义 即时奖励期望： $R(s) = \\mathbb{E}[R_{t+1} \\mid S_t = s]$ 表示从状态 $s$ 出发时立即获得的奖励期望。 回报（Return）： $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$ 其中 $\\gamma \\in [0,1)$ 是折扣因子，用于衡量未来奖励的重要性。 状态价值函数： $V(s) = \\mathbb{E}[G_t \\mid S_t = s]$ 表示从状态 $s$ 开始，遵循当前环境动态所能获得的长期期望回报。 示例说明 以如下状态转移图为例：\n暂时无法在飞书文档外展示此内容\n假设在时刻 $t$ 处于状态 B，则其价值为：$V(B) = p \\cdot (R_C + \\gamma R_D) + (1 - p) \\cdot R_E$\n这体现了状态 $ s $ 的潜在长期收益，包含即时奖励与后续折现奖励。\n2. 推导状态价值函数的贝尔曼方程 我们从定义出发，推导 $V(s)$ 的递推形式。\n$$ \\begin{aligned} V(s) \u0026= \\mathbb{E}[G_t \\mid S_t = s] \\\\ \u0026= \\mathbb{E}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] \\\\ \u0026= \\mathbb{E}[R_{t+1} \\mid S_t = s] + \\gamma \\mathbb{E}[G_{t+1} \\mid S_t = s] \\\\ \u0026= R(s) + \\gamma \\mathbb{E}[G_{t+1} \\mid S_t = s] \\end{aligned} $$ ✅ 注释：此步利用了期望的线性性质。\n进一步处理 $\\mathbb{E}[G_{t+1} \\mid S_t = s]$：\n$$ \\begin{aligned} \\mathbb{E}[G_{t+1} \\mid S_t = s] \u0026= \\sum_{s'} P(s' \\mid s) \\cdot \\mathbb{E}[G_{t+1} \\mid S_t = s, S_{t+1} = s'] \\\\ \u0026= \\sum_{s'} P(s' \\mid s) \\cdot \\mathbb{E}[G_{t+1} \\mid S_{t+1} = s'] \\quad \\text{（马尔可夫性质）} \\\\ \u0026= \\sum_{s'} P(s' \\mid s) \\cdot V(s') \\end{aligned} $$ ✅ 关键点：马尔可夫性质保证了未来期望仅依赖于当前状态，与历史无关。\n代入得最终结果：\n$$ \\boxed{ V(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s) V(s') } $$3. 贝尔曼方程组（引入策略 $\\pi$） 当引入策略 $\\pi(a \\mid s)$ 后，需区分状态价值函数 $V_\\pi(s)$ 和动作价值函数 $Q_\\pi(a, s)$。\n定义：\n$V_\\pi(s)$：在策略 $\\pi$ 下，从状态 $ s $ 出发的期望回报。 $Q_\\pi(a, s)$$：在状态 $$ s $执行动作 $a$ 后，再遵循策略$\\pi$的期望回报。 贝尔曼方程组： $$ \\left\\{ \\begin{aligned} V_\\pi(s) \u0026= \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) Q_\\pi(a, s) \\\\ Q_\\pi(a, s) \u0026= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} P(s', r \\mid a, s) \\left[ r + \\gamma V_\\pi(s') \\right] \\end{aligned} \\right. $$4. 贝尔曼最优方程 最优策略的核心思想\n每一步选择都应最大化未来的期望回报，即 贪心策略在每一步都追求最大值。\n最优价值函数定义\n$V^*(s)$：所有策略中最大的状态价值。 $Q^*(a, s)$：所有策略中最大的动作价值。 贝尔曼最优方程 $$ \\left\\{ \\begin{aligned} V^*(s) \u0026= \\max_a Q^*(a, s) \\\\ Q^*(a, s) \u0026= \\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} P(s', r \\mid a, s) \\left[ r + \\gamma V^*(s') \\right] \\end{aligned} \\right. $$或等价地：\n$$ V^*(s) = \\max_a \\sum_{s'} P(s' \\mid a, s) \\left[ R(s, a, s') + \\gamma V^*(s') \\right] $$总结 概念 公式 说明 状态价值函数（无策略） $V(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s) V(s')$ 基础贝尔曼方程 状态价值函数（有策略） $V_\\pi(s) = \\sum_a \\pi(a \\mid s) Q_\\pi(a, s)$ 策略下期望回报 动作价值函数 $Q_\\pi(a, s) = \\sum_{s'} P(s' \\mid a, s) [ r + \\gamma V_\\pi(s') ]$ 动作执行后期望回报 最优价值函数 $V^*(s) = \\max_a Q^*(a, s)$ 最大可能价值 贝尔曼最优方程 $Q^*(a, s) = \\sum_{s'} P(s' \\mid a, s) [ r + \\gamma V^*(s') ]$ 最优策略基础 这些方程构成了强化学习中动态规划、策略迭代、值迭代等方法的理论基石。\n手推过程 ","permalink":"https://nky10.github.io/theory/rl/%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/","summary":"摘要：贝尔曼方程是强化学习中用于计算状态价值函数的重要工具，通过递归定义，将问题分解为子问题，最终求解最优策略。","title":"贝尔曼方程"},{"content":"一、Q-Learning 1. 前情回顾 状态价值函数与动作-状态价值函数存在如下关系：\n$$ V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a) $$$$ Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma V^\\pi(s')\\right] $$最优策略 $\\pi^*$ 是使得价值函数最大的策略：\n$$ \\pi^* = \\arg\\max_\\pi V^\\pi(s), \\quad \\forall s \\in S $$最优状态价值函数：\n$$ V^*(s) = \\max_\\pi V^\\pi(s) $$最优动作价值函数：\n$$ Q^*(s,a) = \\max_\\pi Q^\\pi(s,a) $$对于最优状态价值函数，我们有：\n$$ V^*(s) = \\max_a Q^*(s,a) $$将Q函数的定义代入：\n$$ V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma V^\\*(s')\\right] $$对于最优的动作价值函数：\n$$ Q^*(s,a) = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma \\max_{a'} Q^\\*(s',a')\\right] $$ $Q^*(s,a)$ 等于：从状态 s 执行动作 a 后，所有可能的下一个状态 s' 所带来的「即时奖励 + 后续最优回报」的期望值。\n这两个式子告诉我们，可以通过迭代计算的方法来更新价值函数。\n2. 算法原理 2.1 基本思想 在智能体与环境的交互中，我们并不知道状态转移概率 $P(s'|s,a)$ 以及奖励 $R(s,a,s')$ 的具体形式，Q-Learning算法通过采样来估计这个期望值。\n我们不再计算所有可能下一个状态 $s'$ 的加权平均，而是通过实际交互，即在状态 $s$ 执行动作 $a$ 到达状态 $s'$ 以及实际获得的奖励 $r$。\n此时的采样目标是：\n$$ r + \\gamma \\max_{a'} Q(s',a') $$注意这里只是当前的价值，不是最优策略下的价值。\n2.2 无偏估计 在给定最优函数 $Q^*$ 的前提下，采样目标 $r + \\gamma \\max_{a'} Q^*(s',a')$ 是 $Q^*(s,a)$ 的一个无偏估计，因为它的期望正好等于 $Q^*(s,a)$。\n无偏估计的定义：\n一个估计量 $X$ 是某个真值 $\\theta$ 的无偏估计，如果：$\\mathbb{E}[X] = \\theta$。\n也就是说，虽然每次采样得到的 $X$ 可能有偏差（比如某次运气好奖励很高），但长期平均来看，它等于真实值。\n证明：\n假设我们现在已经有最优的 $Q^\\*(s,a)$，并且我们从环境采样一次：\n$$ \\text{Sample Target} = r + \\gamma \\max_{a'} Q^\\*(s',a') $$采样目标的期望值：\n$$ \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s',a') \\mid s,a\\right] = \\sum_{s'} P(s'|s,a)\\left[R(s,a,s') + \\gamma \\max_{a'} Q^\\*(s',a')\\right] $$而这正是贝尔曼最优方程右边的内容，那么我们的单次采样能够近似期望值。\n但是，在学习过程中，我们使用的 $Q$ 是估计值，且存在函数逼近误差或采样噪声，因此实际的 TD 目标是有偏的。记住了，Q-learning是自举过程中会引入偏差。\n在随机逼近理论中，如果更新方向是目标值的无偏估计，算法就可以像梯度下降一样，逐步收敛到正确解。如果估计有系统性偏差（有偏），可能收敛到错误的值。\n虽然单次TD目标是有偏的，但随着学习进行，估计误差逐渐减小，偏差趋于零。在满足一定条件下（如充分探索、合适学习率），这种「渐近无偏 + 零均值噪声」的更新机制可以保证算法渐近收敛到最优Q值。\n2.3 时序差分学习（TD-error） 我们不需要一次性更新所有状态，而是每次交互后逐步调整：\n这个逐步交互的过程有点像卡尔曼滤波里计算残差。\n$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right] $$其中：\n$\\alpha$ 是学习率 ($0 \u003c \\alpha \\leq 1$)，需要满足Robbins-Monro 条件 $r + \\gamma \\max_{a'} Q(s',a')$ 是采样目标值(TD) $r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ 是时序差分误差(TD-error) Robbins-Monro 条件（罗宾斯-蒙罗条件）是一组关于学习率序列 $\\{a_t\\}$ 的数学条件，用于保证随机迭代算法能够收敛到目标值（例如最优 Q 值、最优参数等）。\n它最早由 Herbert Robbins 和 Sutton Monro 在 1951 年提出，用于解决「在噪声环境下求根」或「求期望值」的问题。后来被广泛应用于随机逼近（Stochastic Approximation）和强化学习中。\n设 $a_t$ 是第 $t$ 次更新时的学习率（可能依赖于状态-动作对的更新次数），要满足以下两个条件：\n$\\sum_{t=1}^{\\infty} a_t = \\infty$：含义是总「学习力量」是无限的。作用：确保我们有足够的「调整机会」，即使初始值很差，也能最终被修正。 $\\sum_{t=1}^{\\infty} a_t^2 \u003c \\infty$：学习率的「波动能量」是有限的，确保噪声的影响会逐渐被压制，不会导致持续震荡。 在实际编程中通常不会严格按 $1/t$ 调整学习率，因为：\n初期学习太慢 环境可能非平稳 用固定的0.1比较好，虽然不收敛，但是效果不错（qwen说的）。\n虽然固定学习率不满足 Robbins-Monro 收敛条件（会导致震荡），但在实际任务中（如游戏、控制），我们更关心快速学习和良好性能，而非严格收敛。因此常用固定学习率（如 0.1）或自适应方法（如 Adam）。\n于是，我们可以用这个实际采样结果来估计贝尔曼方程右边的值：\n期望近似：用单次采样近似期望值\n$$ \\mathbb{E}\\left[R + \\gamma \\max_{a'} Q^*(s',a')\\right] \\approx r + \\gamma \\max_{a'} Q(s',a') $$ 增量更新：使用增量式方法更新估计值\n$$ Q_{\\text{new}}(s,a) = (1-\\alpha)Q_{\\text{old}}(s,a) + \\alpha\\left[r + \\gamma \\max_{a'} Q(s',a')\\right] $$ 重新整理：\n$$ Q_{\\text{new}}(s,a) = Q_{\\text{old}}(s,a) + \\alpha\\left[r + \\gamma \\max_{a'} Q(s',a') - Q_{\\text{old}}(s,a)\\right] $$ 3. 算法流程 Q-learning 学习的是最优贪婪策略 $\\pi^*(a|s) = \\arg\\max_a Q(s,a)$，而探索使用的是另一个策略（如 $\\varepsilon$-greedy），二者分离，属于典型的 off-policy 方法。\n探索率（Exploration Rate）：指智能体在决策时选择随机动作（而非当前最优动作）的概率。\n最常见的实现是 $\\varepsilon$-greedy 策略：\n以概率 $\\varepsilon$：随机选择一个动作（探索） 以概率 $1-\\varepsilon$：选择当前 Q 值最高的动作（利用） 在实际训练中，通常会让探索率随时间逐渐减小。\nQ-Learning 算法流程 初始化 Q 表（例如，所有 $Q(s,a) = 0$）\n对每一轮 episode：\n从初始状态开始 当未到达终止状态时： 根据当前策略（如 $\\varepsilon$-greedy）选择动作 $a_t$ 为了在探索（exploration）和利用（exploitation）之间平衡，Q-Learning 通常使用 $\\varepsilon$-greedy 策略： 以概率 $\\varepsilon$ 随机选择动作（探索） 以概率 $1-\\varepsilon$ 选择当前 Q 值最大的动作（利用） 随着训练进行，$\\varepsilon$ 可以逐渐减小（$\\varepsilon$ 衰减），实现从探索为主到利用为主的过渡 执行动作，观察奖励 $r_{t+1}$ 和新状态 $s_{t+1}$ 使用 Q-Learning 更新公式更新 Q 值 将状态更新为 $s_{t+1}$ 重复直到 Q 值收敛或达到训练轮数\n4. 总结 Q-learning 是一种离线策略（off-policy）强化学习算法，通过时序差分学习（TD learning）来估计动作价值函数 $Q(s,a)$。它利用一定的行为策略（如 $\\varepsilon$-greedy）收集经验，但是学习一个贪婪的目标策略。算法通过最小化时序差分误差来迭代更新 Q 值。尽管 TD 更新在初期具有偏差，但它是收敛的，并能在策略评估中提供一致估计。\nQ-learning 的本质是：通过 off-policy 的 TD 控制，逐步逼近满足最优贝尔曼方程的动作价值函数。\n局限性 仅适合离散、低维度的空间，当状态-动作空间为高维向量或数量很多时，效率很低 总是在取最大价值下的动作，极易受到噪声的影响 策略是外挂的，难以优化 在一些奖励稀疏的环境下收敛缓慢 二、DQN Deep Q Network (DQN)\n1. 算法原理 与 Q-learning 一样，DQN 希望通过下面的式子迭代动作价值函数：\n$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\right] $$Q-learning 在状态空间巨大（如图像像素）、连续或高维的情况下 Q 表爆炸，无法存储和更新。\n1.1 神经网络 DQN 通过用神经网络近似 Q 函数（进行特征提取）来解决无限（巨大）空间的问题，如图像用 CNN，高维度用 MLP：\n$$ Q(s,a|\\theta) \\approx Q^*(s,a) $$采用均方误差损失函数：\n$$ \\mathcal{L} = \\mathbb{E}\\left[\\text{target} - Q(s,a|\\theta)\\right]^2 = \\mathbb{E}\\left[r + \\gamma \\max_{a'} Q^*(s',a'|\\theta^-) - Q(s,a|\\theta)\\right]^2 $$然而还有两个问题要解决：\n由于 Q 是递推计算的，因此相邻数据具有高相关性，训练过程很震荡，容易陷入局部最优解 在训练神经网络时，我们通常使用随机梯度下降（SGD）或其变体（如 Adam），这些优化方法理论前提是：每个训练样本是独立同分布的 Q 是不断变化的，训练目标不固定，难以收敛 致命三角\n1.2 关键技术 DQN 采用两个策略来缓解上面的问题：\n经验回放（Experience Replay） 创建一个回放缓冲区（Replay Buffer）D，存储所有经验 $(s,a,r,s',\\text{done})$ 每个样本被多次使用，减少样本需求 每次更新时，从 D 中随机采样小批量训练网络 随机能够明显降低数据间的时间相关性 每个样本被多次使用，减少样本需求 DQN 用批量更新更稳定 目标网络（Target Network） 移动靶问题：\n均方误差损失函数中，目标值 target 与当前值 Q 都由 $\\theta$ 控制，而这个损失又去更新 $\\theta$，不容易收敛\nDQN 通过引入目标网络 $\\theta^-$，在一定步数之内不更新（保证这个阶段内不会出现大幅度的震荡）：\n目标网络的更新：每 $C$ 步之后用主网络（评估网络）$\\theta$ 进行替换 主网络的更新：$\\theta$ 每一步都会用均方误差损失去更新 2. 算法流程 1# DQN 算法伪代码 2 3初始化： 4 主Q网络：Q(s, a; θ) 5 目标Q网络：Q(s, a; θ⁻)，初始化 θ⁻ = θ 6 回放缓冲区 D，容量为 M 7 环境状态 s 8 9for episode = 1 to N do: 10 s = 环境初始状态 11 while s 不是终止状态 do: 12 1. 根据ε-greedy策略选择动作a： 13 - 以概率 ε 随机选择动作 14 - 以概率 1-ε 选择 a = argmax_a\u0026#39; Q(s, a\u0026#39;; θ) 15 16 2. 执行动作a，观察奖励r和新状态s\u0026#39; 17 18 3. 存储经验 (s, a, r, s\u0026#39;, done) 到回放缓冲区D 19 20 4. 从D中随机采样一个小批量经验 (s_i, a_i, r_i, s\u0026#39;_i, done_i) 21 22 5. 计算目标Q值： 23 - 如果 s\u0026#39;_i 是终止状态： 24 y_i = r_i 25 - 否则： 26 y_i = r_i + γ * max_{a\u0026#39;} Q(s\u0026#39;_i, a\u0026#39;; θ⁻) 27 28 6. 计算损失函数（均方误差）： 29 L(θ) = E[(y_i - Q(s_i, a_i; θ))²] 30 31 7. 使用梯度下降更新主网络参数θ（如Adam优化器） 32 33 8. 每隔C步，更新目标网络参数：θ⁻ ← θ 34 35 9. s ← s\u0026#39; 36 end while 37end for DQN 改进算法 Double DQN: 解决 DQN 的过估计问题，性能提升显著 Dueling DQN: 分离「状态价值」和「优势」，更适合决策 Prioritized Experience Replay: 重要经验优先回放，加速学习 Noisy Nets: 用网络参数噪声替代 $\\varepsilon$-greedy，实现持续探索 ","permalink":"https://nky10.github.io/theory/rl/%E5%9F%BA%E4%BA%8E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%96%B9%E6%B3%95/","summary":"摘要：基于值函数的强化学习方法，包括Q-Learning和DQN算法原理。","title":"基于值函数的方法"},{"content":"一、信号的调制 在通信系统中，信源输出的是由原始信息变换成的电信号，这种信号通常具有较宽的频谱，并且在频谱的低端分布较大的能量，称为基带信号。但是多数信道是低频端受限的，无法长距离传输低频信号。因此在传输过程中需要将基带信号所蕴含的信息转载到高频载波上，这一过程叫做信号的调制。而在接收端将接收到的信号进行解调，以获取传递的信息。\ngraph LR source(信源)--\u0026gt;mod(调制器)--\u0026gt;road(信道)--\u0026gt;demod(解调器)--\u0026gt;dest(信宿) noise(噪声)--\u0026gt;road 二、调制定理 我们知道一个余弦函数的傅里叶变换为 $$ \\cos(w_0t)\u003c\\frac{Fourier}{}\u003e\\pi [δ(w+w_0)+δ(w-w_0)] $$ 那么一个信号$m(t)$与之相乘，其结果的傅式变换为$\\pi [M(w+w_0)+M(w-w_0)]$，它所表示的物理含义就是是信号$m(t)$的幅度谱$M（\\omega）$分别向高频和低频搬移$\\omega_0$。\n我们将信号$m(t)$看作信源所产生的最高频率为$\\omega_m$低频宽带信号，要使其能够在信道上传输，就可以乘以一个频率高到足以匹配信道的余弦信号（即高频载波），使其所包含的频谱信息都搬移至$[\\omega_0-\\omega_m,\\omega_0+\\omega_m]$的位置，这就是调制定理。\n调制的过程实质是完成信息的转载。\n三、希尔伯特变换 在信号处理领域中，一个实信号的希尔伯特变换(Hilbert transform)是将其通过一个冲激响应为$h(t)=\\frac{1}{\\pi t}$的系统所得到的输出信号。该系统的频率响应为$H(j\\omega)=-sgn(\\omega)$。这种变换所表示的物理含义为信号正频域的部分相移$-\\frac{\\pi}{2}$，信号负频域的部分相移$\\frac{\\pi}{2}$。\n![]\n欧拉公式$e^{j\\omega_0t}=cos(\\omega_0t)+jsin(\\omega_0t)$中我们可以将$cos(\\omega_0t)$与$sin(\\omega_0t)$看作一对希尔伯特变换，而任一实信号$x(t)$均可表示为一系列$e^{j\\omega_0t}$的线性组合，那么$x(t)$与其希尔伯特变换也可以通过这种方式扩展成一个复信号，方便信号的处理。 $$ \\tilde{x}=x(t)+j\\hat{x}(t) $$四、常规调幅（AM） 常规调幅（Conventional Amplitude Modulation）的过程是将基带信号m(t)的电平抬高A0再乘以高频载波***c(t)***实现频谱的搬移。\n其数学表达式为 $$ s_{AM}(t)=(A_0+m(t))·c(t) $$1. 理论分析 基带调制信号***m(t)***在常规调幅的过程中要保证加入抬高电平$A_0$之后信号的任意一个时刻的幅度要大于零，即$|A_0|\u003e=|m(t)|_{max}$。这里引入一个参数 调幅深度β，即$β_{AM}=\\frac{|m(t)|_{max}}{A_0}$。为了保证不出现”过调幅”的现象，调幅深度应满足β=\u0026lt;1(一般取小于0.8的值)。\n下面以单音信号为例进行分析。\n设调制信号$m(t)=A_mcos(\\omega_mt)$，抬高电平为$A_0$，载波为$c(t)=cos(\\omega_0t)$。则已调信号为 $$ \\begin{align} s_{AM}(t)\u0026=(A_0+m(t))·c(t)\\\\ \u0026=(A_0+A_mcos(w_mt))·cos(w_0t)\\\\ \u0026=A_0cos(w_0t)+\\frac{A_m}{2}\\{cos[(w_m+w_0)t]+[(w_m-w_0)t]\\} \\end{align} $$ 其频域表达式为 $$ \\begin{align} S_{AM}(\\omega)=\\pi A_0\u0026[δ(w+w_0)+δ(w-w_0)]\\\\\u0026+\\frac{\\pi A_m}{2}[δ(w+w_0+w_m)+δ(w-w_0-w_m)+δ(w+w_0-w_m)+δ(w-w_0+w_m)] \\end{align} $$即$A_0+m(t)的频谱被搬移到\\omega_0处$，那么$S_{AM}(\\omega)$的图像应该是\n2. AM仿真 针对这一过程我们在MATLAB中进行仿真验证，结果如下：\n这与我们的理论推导结果相符合。\n仔细观察我们可以看到仿真后频域的图像中所出现的并非是冲激信号，中间会留有空隙。\n这是因为在计算机信号都是数字的形式，我们无法直接对模拟信号进行处理，只能对模拟信号进行采样、量化、编码，将其近似为数字信号，而在这一过程中发生的频谱泄漏现象即为图中“冲激信号”的留白部分，我们可以通过增加采样点数来抑制这一现象。\n3. 包络检波 在整个通信系统中我们不能只考虑信源发出的信号信道能否传输，还要考虑信宿如何去从已调信号中获取我们传递的信息。\n常规调幅的过程其实就是将调制信号所包含的信息通过载波转载后存放在已调信号的幅度中，在MATLAB仿真中我们绘制出了已调信号时域波形的包络，可以发现其包络就是我们的调制信号。而信宿要接收的信号是从这一已调信号通过解调器解析出来。这里解调器的工作就是获取这一包络。\n已调信号的有上下两个包络，它们是一样的（只是幅度正负不同），我们只要其一即可，这里我们可以通过半波整流来达到这一效果。随后通过一个低通滤波器，将已调信号的高频分量（来自载波）被滤去，就会只剩下低频的基带信号。包络检波可以用一种非常简单的电路来实现。\n4. “过调幅”现象 前面我们提到AM的调幅深度不能小于1，这是因为获取的已调信号的包络是上下对称的，当$β_{AM}\u003e1$时，我们并不能保证信号在任意时刻的幅度都大于零，在调制之后中是没问题的，但是在包络检波的第一步半波整流之后就出错了。在这一步我们发现信号的包络线与调制信号并不相符，所展示出的部分是上下两个包络线大于零的部分即二者之间发生了相互干扰。\n加入抬高电平$A_0$并使调幅深度小于1就是为了让二者不发生干扰以保证后续能够解调出原信号波形。\n5. 希尔伯特变换与解析包络 虽然我们在前面阐述了包络检波的一种简单思路，它可以通过一些简单的模拟器件实现，但是我们难以通过一些数学表达式来分析检波的这一过程。这里我们可以用希尔伯特变换进行分析。\n对于我们的基带信号$m(t)$，它经过常规调幅之后所得到的输出信号为$s_{AM}(t)=(A_0+m(t))·cos(\\omega_0t)$。这里我们将$(A_0+m(t))$看作一个整体$x(t)$，将$s_{AM}(t)$与其希尔伯特变换构建成一个复信号，即 $$ s_{AM}(t)+j\\hat{s}_{AM}(t)=x(t)·cos(\\omega_0t)+jx(t)·sin(\\omega_0t)=x(t)·e^{jw_0t} $$那么这个复信号的幅值就是$x(t)$的包络，在MATLAB中可以用*abs()*函数取其幅值。\n需要注意的是在MATLAB中希尔伯特变换的函数*Hilbert()*所得到的是该实信号与其希式变换构造出的复信号而不只是一个变换。\n用这种方式解析出的信号如下。\n可以看出我们的基带信号已被成功解调，但是端点处有失真（另一端也有，这里坐标轴限定了长度，后半部分未显示）。\n从我们的理论推导来看，这种方式应该是无失真解调，但是这里却出现了失真，这一问题我们称之为端点效应。\n1\t由于数字实现过程涉及与原始信号构造相位差为Π/2的共轭信号，而共轭信号通过“Fourier变换-双边谱对折为单边谱-Fourier逆变换”求取，对周期信号进行非完整周期采样时，Fourier变换将引起Gibbs现象，发生频率泄露。而对单边谱进行Fourier变换的逆变换时，这种频率泄露造成的误差无法抵消，导致信号两端产生“飞逸”，使求取的Hilbert谱无法准确反映原始信号的本质特性。\t​\t引自\n7. MATLAB代码 1Am = 1; %调制信号幅度 2fm = 5; %调制信号频率 3beta = 0.5; %调幅深度 4f0 = 50;%载波频率 5A0 = Am/beta;%抬高电平 6%{ 常规调幅 %} 7%{ 8fs = 500; %大于二倍最高频率 9T = 1; 10dt = 0.0001; 11N = T/dt; %采样点数 12t = [0:N-1] * dt; %采样点的时间序列 13 14dt与fs应该成倒数关系，直接设定容易错 15%} 16fs = 500; %采样频率 17T = 1; 18N = 8192; %采样点数 N是fs整数倍的时候不会出现端点效应 ？？？ 19n = 0:N-1; %采样点的时间序列 20t = n/fs; 21 22m = Am*cos(2*pi*fm*t);%调制信号 23c = cos(2*pi*f0*t);%载波 24 25s = (A0+m).*c; 26 27figure(1); 28 29subplot(4,1,1) 30plot(t,m) 31axis([0 1 -4 4]) 32line([0,1],[0,0],\u0026#39;color\u0026#39;,\u0026#39;r\u0026#39;); 33%line画线函数[a,b],[c,d]是从（a,c）画到（b,d） 34title(\u0026#39;调制信号m(t)\u0026#39;) 35 36subplot(4,1,2) 37plot(t,c) 38axis([0 1 -4 4]) 39line([0,1],[0,0],\u0026#39;color\u0026#39;,\u0026#39;r\u0026#39;); 40title(\u0026#39;高频载波c(t)\u0026#39;) 41 42subplot(4,1,3) 43plot(t,s) 44axis([0 1 -4 4]) 45line([0,1],[0,0],\u0026#39;color\u0026#39;,\u0026#39;r\u0026#39;); 46hold on 47plot(t,A0+m,\u0026#39;r--\u0026#39;) %绘制包络线 48hold on 49plot(t,-(A0+m),\u0026#39;r--\u0026#39;) %绘制包络线 50title(\u0026#39;AM已调信号s(t)\u0026#39;) 51 52%{ 53 包络检波（非相干解调） 54 半波整流-\u0026gt;低通滤波-\u0026gt;隔断直流 55%} 56s_banbo = s; 57s_banbo(s_banbo\u0026lt;0)=0; %将小于零的部分置零 58 59subplot(4,1,4) 60plot(t,s_banbo) 61axis([0 1 -4 4]) 62line([0,1],[0,0],\u0026#39;color\u0026#39;,\u0026#39;r\u0026#39;); 63hold on 64plot(t,A0+m,\u0026#39;r--\u0026#39;) %绘制包络线 65title(\u0026#39;半波整流后\u0026#39;) 66 67figure(2) 68 69subplot(4,1,1) 70n=(-N/2):1:(N/2 -1); 71f = (n-1).*fs/N; 72plot(f,fftshift(abs((2/N)*fft(A0+m)))) 73axis([-110 110 0 4]) 74title(\u0026#39;A0+m(t)幅度谱\u0026#39;) 75subplot(4,1,2) 76plot(f,fftshift(abs((2/N)*fft(c)))) 77axis([-110 110 0 2]) 78title(\u0026#39;载波信号幅度谱\u0026#39;) 79subplot(4,1,3) 80plot(f,fftshift(abs((2/N)*fft(s)))) 81axis([-110 110 0 4]) 82title(\u0026#39;已调信号幅度谱\u0026#39;) 83subplot(4,1,4) 84plot(f,fftshift(abs((2/N)*fft(s_banbo)))) 85axis([-110 110 0 2]) 86title(\u0026#39;半波整流后信号幅度谱\u0026#39;) 87figure(3) 88s_m =abs(hilbert(s)); 89 % matlab的hilbert得到的是构建出的复信号而不是其希尔伯特 90plot(t,s_m-mean(s_m)); %均值（直流） 91axis([0 1 -4 4]) 92title(\u0026#39;解调出的信号为\u0026#39;) 五、双边带调幅（DSB） 双边带调幅（Double Side Band AM）\n六、单边带条调幅（SSB） 单边带调幅（Single Side Band AM ）\n七、残留边带调幅（VSB） 残留边带调幅（Vestigial Side Band AM）\n​\n","permalink":"https://nky10.github.io/anything/%E5%B9%85%E5%BA%A6%E8%B0%83%E5%88%B6am/","summary":"介绍幅度调制的基本原理，分析了频谱泄漏和过调幅现象。","title":"幅度调制AM"}]